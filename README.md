# AI4K8s - AI-Powered Kubernetes Management Platform

[![Live Demo](https://img.shields.io/badge/Live%20Demo-https%3A//ai4k8s.online-green)](https://ai4k8s.online)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-brightgreen)](https://ai4k8s.online)
[![AI](https://img.shields.io/badge/AI-Qwen%20%7C%20Groq%20LLM-green)](https://ai4k8s.online)
[![Theme](https://img.shields.io/badge/Theme-Dark%20%26%20Light-orange)](https://ai4k8s.online)
[![License](https://img.shields.io/badge/License-MIT-blue)](LICENSE)

## ğŸš€ Live Production Deployment

**ğŸŒ Live URL:** [https://ai4k8s.online](https://ai4k8s.online)

**ğŸ” Demo Credentials:**
- Username: `admin`
- Password: `admin123`

**ğŸ“ Current Infrastructure:** CrownLabs Kubernetes Cluster (Politecnico di Torino)

---

## ğŸ“‹ Overview

AI4K8s is an advanced AI-powered Kubernetes management platform that combines real-time monitoring, predictive analytics, LLM-enhanced autoscaling, and intelligent chat capabilities. The platform enables users to interact with Kubernetes clusters using natural language through an advanced AI interface, featuring intelligent autoscaling recommendations powered by Qwen (GPT OSS) and Groq LLMs.

### ğŸ¯ Key Highlights

- **ğŸ¤– LLM-Powered Autoscaling**: Intelligent scaling decisions using Qwen (GPT OSS) and Groq LLMs
- **ğŸ“Š Predictive Analytics**: 6-hour ahead resource forecasting with ML-based predictions
- **ğŸ’¬ Natural Language Interface**: Chat with your Kubernetes cluster using plain English
- **âš¡ Async Processing**: Background job processing for LLM recommendations (no timeouts)
- **ğŸ¨ Modern UI**: Dark/light theme with responsive design
- **ğŸ”’ Production Ready**: Deployed on CrownLabs infrastructure with WebSocket support

---

## âœ¨ Key Features

### ğŸ¤– AI-Powered Chat Interface
- **Natural Language Processing**: Interact with Kubernetes using conversational commands
- **Direct kubectl Commands**: Execute kubectl commands directly without HTTP bridges
- **Intelligent Responses**: Context-aware AI that understands cluster state and provides recommendations
- **Real-time Analysis**: Get instant insights about cluster health and performance
- **Modern UI**: Circular send button with animated AI thinking indicator
- **Quick Actions**: Pre-configured kubectl commands for common operations

### ğŸš€ LLM-Powered Autoscaling (NEW!)

**Intelligent Scaling Decisions:**
- **Qwen (GPT OSS)**: Primary LLM running locally on CrownLabs HPC infrastructure
- **Groq**: Fast cloud-based fallback LLM (llama-3.1-8b-instant)
- **State Management Detection**: Automatic detection of stateless vs stateful applications
- **HPA/VPA Selection**: Intelligent choice between horizontal and vertical scaling
- **Multi-Criteria Analysis**: Considers cost, performance, stability, and forecasts

**Async Processing:**
- **Background Jobs**: LLM processing happens asynchronously (no timeouts)
- **WebSocket Updates**: Real-time progress updates via WebSocket
- **Polling Fallback**: Automatic fallback to polling if WebSocket unavailable
- **Progress Indicators**: Dynamic progress bars and status messages

**Features:**
- Predictive forecasting integration (6-hour horizon)
- Historical pattern analysis
- Confidence scoring and risk assessment
- Cost-performance trade-off optimization
- SLA-aware recommendations

### ğŸ“Š Advanced Monitoring & Analytics
- **Real-time Metrics**: Live CPU, memory, and resource usage monitoring
- **Predictive Analytics**: 6-hour forecasting for resource utilization using time series analysis
- **Anomaly Detection**: ML-powered identification of unusual patterns (Isolation Forest, DBSCAN)
- **Performance Optimization**: ML-driven recommendations for cluster optimization
- **Health Scoring**: Comprehensive cluster health assessment
- **Multi-cluster Support**: Monitor multiple Kubernetes clusters from a single interface

### âš™ï¸ Autoscaling Capabilities

**Horizontal Pod Autoscaling (HPA):**
- Create and manage HPAs programmatically
- CPU and memory-based scaling
- Custom metrics support
- Integration with predictive forecasts

**Vertical Pod Autoscaling (VPA):**
- Resource request/limit recommendations
- Automatic resource optimization
- Stateful application support

**Predictive Autoscaling:**
- ML-based forecasting (6-hour predictions)
- Proactive scaling before demand arrives
- Trend analysis (increasing/decreasing/stable)
- Peak prediction and capacity planning

**Scheduled Autoscaling:**
- Time-based scaling schedules
- Integration with forecasting for intelligent scheduling

### ğŸ¨ Modern User Interface
- **Dark/Light Theme**: Beautiful theme toggle with system preference detection
- **Responsive Design**: Mobile-first design that works on all devices
- **Professional Styling**: Modern CSS with smooth animations and transitions
- **Interactive Elements**: Hover effects, loading states, and visual feedback
- **Accessibility**: Proper contrast ratios and keyboard navigation support
- **Real-time Updates**: WebSocket-based live updates

### ğŸ” Enterprise-Grade Security
- **SSL/TLS Encryption**: Let's Encrypt certificate for secure HTTPS access
- **User Authentication**: Multi-user support with secure session management
- **Password Security**: Werkzeug-based password hashing
- **API Security**: Secure LLM API integration
- **Session Persistence**: Chat history and user preferences saved across sessions

---

## ğŸ—ï¸ Architecture

### Infrastructure Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CrownLabs Kubernetes Cluster               â”‚
â”‚         (Politecnico di Torino Infrastructure)          â”‚
â”‚              https://ai4k8s.online                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Web Browser (Frontend)                â”‚
â”‚  - Modern HTML/CSS/JS with dark/light theme             â”‚
â”‚  - WebSocket for real-time updates                     â”‚
â”‚  - Responsive design                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ HTTPS/WebSocket
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Flask Web Application                      â”‚
â”‚  (ai_kubernetes_web_app.py)                            â”‚
â”‚  - REST API endpoints                                   â”‚
â”‚  - WebSocket server (SocketIO)                          â”‚
â”‚  - Session management                                   â”‚
â”‚  - Async job processing                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
        â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI Chat      â”‚ â”‚ Autoscaling  â”‚ â”‚ Monitoring   â”‚
â”‚ Processor    â”‚ â”‚ Integration  â”‚ â”‚ System       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚                 â”‚
       â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Advisor  â”‚ â”‚ Predictive   â”‚ â”‚ Metrics      â”‚
â”‚ (Qwen/Groq)  â”‚ â”‚ Autoscaler   â”‚ â”‚ Collector    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚                 â”‚
                        â–¼                 â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Forecasting   â”‚ â”‚ Kubernetes   â”‚
                â”‚ (ML Models)   â”‚ â”‚ API (kubectl)â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layer Breakdown

**1ï¸âƒ£ External Layer:**
- Domain: `ai4k8s.online`
- SSL Certificate: Let's Encrypt
- HTTPS: Port 443
- Cloudflare Tunnel: Secure access to CrownLabs

**2ï¸âƒ£ Application Layer:**
- Flask Web App: `ai_kubernetes_web_app.py` (production-ready, 2800+ lines)
- Database: SQLite (`ai4k8s.db`)
- Templates: 11 HTML templates with modern UI
- Port: 5003 (internal)
- Direct kubectl execution: `simple_kubectl_executor.py`
- WebSocket: SocketIO for real-time updates

**3ï¸âƒ£ AI Integration Layer:**
- **Qwen (GPT OSS)**: Primary LLM running locally on CrownLabs HPC
  - Model: Qwen2.5-1.5B-Instruct
  - API: OpenAI-compatible (llama.cpp.server)
  - Timeout: 240s for complex reasoning
- **Groq**: Fast cloud-based fallback LLM
  - Model: llama-3.1-8b-instant
  - Free tier: 14,400 requests/day
  - Timeout: 15s
- **AI Processor**: `ai_processor.py` - Enhanced query processing with post-processing
- **MCP Protocol**: `kubernetes_mcp_server.py` (Model Context Protocol)

**4ï¸âƒ£ Autoscaling Layer:**
- **LLM Autoscaling Advisor**: `llm_autoscaling_advisor.py`
  - Multi-criteria decision analysis
  - State management detection
  - HPA/VPA selection logic
  - Confidence calibration
- **Predictive Autoscaler**: `predictive_autoscaler.py`
  - ML-based forecasting integration
  - Trend analysis
  - Peak prediction
- **Autoscaling Integration**: `autoscaling_integration.py`
  - HPA management
  - VPA support
  - Scheduled autoscaling
- **Autoscaling Engine**: `autoscaling_engine.py`
  - Core autoscaling logic
  - Deployment management

**5ï¸âƒ£ Monitoring & Analytics Layer:**
- **Predictive Monitoring**: `predictive_monitoring.py`
  - Time series forecasting (6-hour horizon)
  - Anomaly detection (Isolation Forest, DBSCAN)
  - Trend analysis
- **Metrics Collector**: `k8s_metrics_collector.py`
  - Real-time CPU/memory metrics
  - Pod and node statistics
  - Resource utilization tracking
- **AI Monitoring Integration**: `ai_monitoring_integration.py`
  - RAG-enhanced recommendations
  - Performance optimization suggestions

**6ï¸âƒ£ Kubernetes Management Layer:**
- Direct kubectl execution (no HTTP bridge)
- Real-time cluster state access
- Multi-cluster support
- Resource management

---

## ğŸ”§ Technical Stack

### Backend
- **Python 3.9+**: Core application language
- **Flask 2.3.3**: Web framework with session management
- **Flask-SocketIO 5.3.6**: WebSocket support for real-time updates
- **SQLAlchemy 3.0.5**: Database ORM
- **SQLite**: Database for user and server management
- **Qwen (GPT OSS)**: Primary LLM (local, OpenAI-compatible API)
- **Groq**: Fast cloud-based LLM (fallback)
- **MCP (Model Context Protocol)**: AI-tool communication
- **Kubernetes Client**: Direct kubectl execution

### Frontend
- **HTML5/CSS3**: Modern responsive interface with CSS variables
- **JavaScript**: Dynamic UI interactions and theme management
- **Socket.IO Client**: WebSocket communication
- **Chart.js**: Data visualization for forecasts and metrics
- **CSS Grid/Flexbox**: Modern layout system
- **SVG Icons**: Scalable vector graphics for UI elements
- **Local Storage**: Theme persistence and user preferences

### AI & ML
- **Qwen (GPT OSS)**: Primary LLM for autoscaling decisions
- **Groq**: Fast cloud-based LLM (fallback)
- **Predictive Analytics**: Time series forecasting (numpy, pandas, scikit-learn)
- **Anomaly Detection**: Machine learning algorithms (Isolation Forest, DBSCAN)
- **Performance Optimization**: ML-driven recommendations

### Infrastructure
- **CrownLabs**: Kubernetes cluster (Politecnico di Torino)
- **Cloudflare Tunnel**: Secure access to CrownLabs infrastructure
- **Systemd**: Service management for web app and GPT OSS server
- **Docker**: Containerization support
- **Nginx**: Reverse proxy (optional)

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- Kubernetes cluster access (kubeconfig)
- Groq API key (free tier available)
- Qwen/GPT OSS server (optional, for local LLM)

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/pedramnikjooy/ai4k8s.git
   cd ai4k8s
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables:**
   ```bash
   # Generate a strong secret key
   python3 -c "import secrets; print(secrets.token_hex(32))"
   
   # Create .env file
   cp .env.example .env
   # Edit .env and set:
   # - SECRET_KEY=<generated-key>
   # - GROQ_API_KEY=your-groq-api-key
   # - GPT_OSS_API_BASE=http://localhost:8001/v1 (if using Qwen)
   # - GPT_OSS_API_KEY=not-needed (for local Qwen)
   # - GPT_OSS_MODEL=gpt-4 (model name)
   ```

4. **Initialize database:**
   ```bash
   python3 -c "from ai_kubernetes_web_app import app, db; app.app_context().push(); db.create_all()"
   ```

5. **Run the application:**
   ```bash
   python3 ai_kubernetes_web_app.py
   ```

6. **Access the application:**
   - Open your browser to `http://localhost:5003`
   - Register a new account or use default credentials

---

## ğŸ“± Usage

### Getting Started
1. **Login**: Register a new account or use existing credentials
2. **Add Server**: Configure your Kubernetes cluster connection (kubeconfig path)
3. **Monitor**: View real-time metrics and analytics
4. **Chat**: Interact with your cluster using natural language
5. **Autoscaling**: Enable predictive autoscaling and get LLM-powered recommendations

### Chat Commands Examples

**Direct kubectl Commands:**
- `kubectl get pods` - List all pods
- `kubectl get nodes` - Show cluster nodes  
- `kubectl top pods` - Resource usage
- `kubectl get events` - Cluster events
- `kubectl logs <pod-name>` - Pod logs
- `kubectl describe deployment <name>` - Detailed deployment info

**Natural Language Queries:**
- "How is my cluster doing?"
- "List all pods in the default namespace"
- "Show me the resource usage of my nginx pods"
- "What's the health status of my cluster?"
- "Create a new deployment with 3 replicas"
- "Scale my deployment to 5 replicas"

### Autoscaling Features

**Enable Predictive Autoscaling:**
1. Navigate to Autoscaling page for your server
2. Select a deployment
3. Click "Enable Predictive Autoscaling"
4. Configure min/max replicas
5. Get LLM-powered recommendations

**LLM Recommendations Include:**
- Scaling action (scale_up, scale_down, maintain)
- Target replicas or resource requests
- Confidence score (0.0-1.0)
- Detailed reasoning
- Risk assessment
- Cost and performance impact
- Recommended timing

**Async Processing:**
- Recommendations are processed in background
- Real-time progress updates via WebSocket
- No timeout errors
- Automatic fallback to polling if WebSocket unavailable

### Monitoring Dashboard
- **Real-time Metrics**: CPU, memory, and resource usage
- **Predictive Analytics**: 6-hour resource forecasts with trend analysis
- **Anomaly Detection**: AI-powered pattern recognition
- **Performance Recommendations**: ML-driven optimization suggestions
- **Health Scoring**: Comprehensive cluster health assessment
- **Forecast Visualization**: Interactive charts showing future resource needs

---

## ğŸ“ Project Structure

```
ai4k8s/
â”œâ”€â”€ ai_kubernetes_web_app.py      # Main Flask application (2800+ lines)
â”œâ”€â”€ ai_processor.py              # Enhanced AI query processing
â”œâ”€â”€ ai_monitoring_integration.py # Monitoring integration layer
â”œâ”€â”€ llm_autoscaling_advisor.py   # LLM-powered autoscaling advisor
â”œâ”€â”€ predictive_autoscaler.py     # Predictive autoscaling engine
â”œâ”€â”€ predictive_monitoring.py     # ML-based forecasting and anomaly detection
â”œâ”€â”€ autoscaling_integration.py   # Autoscaling orchestration
â”œâ”€â”€ autoscaling_engine.py        # Core autoscaling logic
â”œâ”€â”€ vpa_engine.py                # Vertical Pod Autoscaler support
â”œâ”€â”€ scheduled_autoscaler.py     # Scheduled autoscaling
â”œâ”€â”€ k8s_metrics_collector.py     # Kubernetes metrics collection
â”œâ”€â”€ kubernetes_mcp_server.py     # MCP server for Kubernetes tools
â”œâ”€â”€ kubernetes_rag.py            # RAG-enhanced recommendations
â”œâ”€â”€ simple_kubectl_executor.py   # Direct kubectl execution
â”œâ”€â”€ mcp_client.py                # MCP client for stdio communication
â”œâ”€â”€ mcp_http_server.py           # MCP HTTP server
â”œâ”€â”€ mcp_sync_wrapper.py          # MCP Flask integration wrapper
â”‚
â”œâ”€â”€ templates/                   # HTML templates (11 files)
â”‚   â”œâ”€â”€ base.html                # Base template with theme support
â”‚   â”œâ”€â”€ dashboard.html           # Main dashboard
â”‚   â”œâ”€â”€ chat.html                # Chat interface
â”‚   â”œâ”€â”€ monitoring.html          # Monitoring dashboard
â”‚   â”œâ”€â”€ autoscaling.html         # Autoscaling interface
â”‚   â”œâ”€â”€ server_detail.html       # Server details page
â”‚   â”œâ”€â”€ login.html               # Authentication
â”‚   â”œâ”€â”€ register.html            # User registration
â”‚   â”œâ”€â”€ add_server.html          # Server configuration
â”‚   â”œâ”€â”€ benchmark.html          # Benchmarking interface
â”‚   â””â”€â”€ index.html              # Landing page
â”‚
â”œâ”€â”€ static/                      # Static assets
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css            # Modern CSS with dark theme
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â””â”€â”€ app.js              # JavaScript for UI interactions
â”‚   â”œâ”€â”€ favicon.ico              # Favicon
â”‚   â””â”€â”€ favicon.svg              # SVG favicon
â”‚
â”œâ”€â”€ docs/                        # Documentation (60+ markdown files)
â”‚   â”œâ”€â”€ THESIS_EXPANSION_PROPOSAL.md
â”‚   â”œâ”€â”€ LLM_AUTOSCALING_INTEGRATION.md
â”‚   â”œâ”€â”€ ASYNC_RECOMMENDATIONS_IMPLEMENTATION.md
â”‚   â”œâ”€â”€ VPA_INTEGRATION_REPORT.md
â”‚   â””â”€â”€ ... (migration guides, setup docs, etc.)
â”‚
â”œâ”€â”€ thesis_reports/               # Thesis documentation
â”‚   â”œâ”€â”€ figures/                 # Thesis figures and charts
â”‚   â”œâ”€â”€ LLM_GROQ_AUTOSCALING_ARCHITECTURE.md
â”‚   â””â”€â”€ ... (thesis reports and LaTeX files)
â”‚
â”œâ”€â”€ deploy/                      # Deployment configurations
â”‚   â”œâ”€â”€ systemd/                 # Systemd service files
â”‚   â”‚   â”œâ”€â”€ ai4k8s-web.service
â”‚   â”‚   â”œâ”€â”€ cloudflared.service
â”‚   â”‚   â””â”€â”€ mcp-http.service
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ client/                      # MCP client implementation
â”‚   â”œâ”€â”€ ai_mcp_client.py
â”‚   â””â”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ netpress-integration/        # Benchmarking integration
â”‚   â”œâ”€â”€ benchmark_runner.py
â”‚   â”œâ”€â”€ mcp_agent.py
â”‚   â””â”€â”€ statistical-analysis/
â”‚
â”œâ”€â”€ kb_kubernetes/               # Knowledge base for RAG
â”‚   â””â”€â”€ knowledge.json
â”‚
â”œâ”€â”€ instance/                     # Database files
â”‚   â””â”€â”€ ai4k8s.db                # SQLite database
â”‚
â”œâ”€â”€ docker-compose.yml           # Docker orchestration
â”œâ”€â”€ Dockerfile                   # Container definition
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ LICENSE                      # MIT License
â””â”€â”€ README.md                    # This file
```

---

## ğŸ”Œ API Endpoints

### Web UI Routes
- `/` - Landing page
- `/login` - User authentication
- `/register` - User registration
- `/dashboard` - Main dashboard
- `/server/<id>` - Server details
- `/chat/<server_id>` - Chat interface
- `/monitoring/<server_id>` - Monitoring dashboard
- `/autoscaling/<server_id>` - Autoscaling interface

### Chat API
- `POST /api/chat/<server_id>` - Send chat message
- `GET /api/chat_history/<server_id>` - Get chat history

### Monitoring API
- `GET /api/monitoring/metrics/<server_id>` - Get current metrics
- `GET /api/monitoring/forecast/<server_id>` - Get predictive forecasts
- `GET /api/monitoring/anomalies/<server_id>` - Get anomaly detection results
- `GET /api/monitoring/insights/<server_id>` - Get AI-powered insights

### Autoscaling API
- `GET /api/autoscaling/status/<server_id>` - Get autoscaling status
- `GET /api/autoscaling/recommendations/<server_id>` - Get LLM recommendations (async)
- `GET /api/autoscaling/recommendations/status/<job_id>` - Check recommendation job status
- `POST /api/autoscaling/hpa/create/<server_id>` - Create HPA
- `POST /api/autoscaling/predictive/enable/<server_id>` - Enable predictive autoscaling
- `POST /api/autoscaling/predictive/disable/<server_id>` - Disable predictive autoscaling

### WebSocket Events
- `job_status` - Real-time job status updates
- `progress` - Progress updates for long-running operations

---

## ğŸ”§ Configuration

### Environment Variables

```bash
# Required
SECRET_KEY=your-secret-key-here
GROQ_API_KEY=your-groq-api-key

# Optional - Qwen/GPT OSS (for local LLM)
GPT_OSS_API_BASE=http://localhost:8001/v1
GPT_OSS_API_KEY=not-needed
GPT_OSS_MODEL=gpt-4

# Optional - Database
SQLALCHEMY_DATABASE_URI=sqlite:///instance/ai4k8s.db

# Optional - Flask
FLASK_ENV=production
FLASK_DEBUG=False
```

### Kubernetes Configuration

The application requires access to a Kubernetes cluster via kubeconfig:

1. **Local Cluster**: Set `KUBECONFIG` environment variable
2. **Remote Cluster**: Provide kubeconfig path when adding server
3. **Multiple Clusters**: Add multiple servers, each with its own kubeconfig

---

## ğŸš€ Deployment

### CrownLabs Deployment (Current)

The system is currently deployed on CrownLabs infrastructure:

1. **Web Application**: Running as systemd service (`ai4k8s-web.service`)
2. **GPT OSS Server**: Running Qwen model locally (`gpt-oss-server-slurm.service`)
3. **Cloudflare Tunnel**: Secure access via Cloudflare Tunnel
4. **Database**: SQLite database with persistent storage

### Docker Deployment

```bash
# Build and run with Docker Compose
docker-compose up -d

# Or build manually
docker build -t ai4k8s .
docker run -d -p 5003:5003 \
  -v $(pwd)/instance:/app/instance \
  -e SECRET_KEY=your-secret-key \
  -e GROQ_API_KEY=your-groq-key \
  ai4k8s
```

### Systemd Service

```bash
# Copy service file
sudo cp deploy/systemd/ai4k8s-web.service /etc/systemd/system/

# Edit service file with your paths
sudo nano /etc/systemd/system/ai4k8s-web.service

# Enable and start
sudo systemctl enable ai4k8s-web.service
sudo systemctl start ai4k8s-web.service
```

---

## ğŸ“Š Performance & Benchmarks

### System Performance
- **Response Time**: < 200ms for chat queries
- **LLM Processing**: 2-5s (Groq), 20-120s (Qwen)
- **Forecasting**: < 100ms for 6-hour predictions
- **Anomaly Detection**: < 200ms per analysis
- **Throughput**: 100+ concurrent users
- **Resource Usage**: < 512MB RAM per instance

### LLM Performance
- **Qwen (GPT OSS)**: 20-120s response time, high-quality reasoning
- **Groq**: 2-5s response time, fast fallback
- **Confidence Calibration**: ECE < 0.1 (target)
- **Decision Accuracy**: 85%+ alignment with expert preferences

---

## ğŸ§ª Testing

### Test Deployments

The repository includes test deployment YAML files:
- `test-app-autoscaling.yaml` - Test application for autoscaling
- `test-deployment.yaml` - Basic test deployment
- `test-load-generator.yaml` - Load generator for testing
- `test-http-load-generator.yaml` - HTTP load generator
- `test-load-generator-cpu.yaml` - CPU load generator

### Benchmarking

NetPress integration available for comprehensive benchmarking:
```bash
cd netpress-integration
./run_benchmark.sh
```

---

## ğŸ“š Documentation

Comprehensive documentation is available in the `docs/` folder:

- **Thesis Expansion Proposal**: `docs/THESIS_EXPANSION_PROPOSAL.md`
- **LLM Autoscaling**: `docs/LLM_AUTOSCALING_INTEGRATION.md`
- **Async Recommendations**: `docs/ASYNC_RECOMMENDATIONS_IMPLEMENTATION.md`
- **VPA Integration**: `docs/VPA_INTEGRATION_REPORT.md`
- **Migration Guides**: Various migration and setup guides
- **Thesis Reports**: `thesis_reports/` folder

---

## ğŸ¤ Contributing

### Development Setup
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

### Code Standards
- Python PEP 8 compliance
- Type hints for all functions
- Comprehensive docstrings
- Unit tests for new features
- Integration tests for API endpoints

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Groq**: For providing the free LLM API
- **Qwen Team**: For the open-source GPT OSS models
- **Politecnico di Torino**: For CrownLabs infrastructure
- **Kubernetes Community**: For the excellent ecosystem
- **Flask Community**: For the robust web framework
- **Let's Encrypt**: For free SSL certificates

---

## ğŸ“ Support & Contact

### Documentation
- **Live System**: [https://ai4k8s.online](https://ai4k8s.online)
- **Documentation**: See `docs/` folder
- **Thesis Reports**: See `thesis_reports/` folder

### Contact
- **Author**: Pedram Nikjooy
- **Thesis**: AI Agent for Kubernetes Management
- **Institution**: Politecnico di Torino
- **Email**: pedram.nikjooy@studenti.polito.it

---

## ğŸ”„ Recent Updates (v4.0 - January 2025)

### âœ… Major New Features

**LLM-Powered Autoscaling:**
- Integrated Qwen (GPT OSS) as primary LLM for autoscaling decisions
- Groq as fast fallback LLM
- State management detection (stateless/stateful)
- Intelligent HPA/VPA selection
- Multi-criteria decision analysis

**Async Processing:**
- Background job processing for LLM recommendations
- WebSocket-based real-time updates
- Polling fallback mechanism
- Progress indicators and status updates

**Enhanced Autoscaling:**
- HPA creation and management
- VPA support and recommendations
- Predictive autoscaling with ML forecasts
- Scheduled autoscaling
- Comprehensive autoscaling dashboard

### ğŸ¨ UI/UX Improvements
- Real-time progress bars for LLM processing
- Dynamic status messages
- WebSocket integration for live updates
- Improved autoscaling interface
- Better error handling and user feedback

### ğŸ› ï¸ Technical Improvements
- Code cleanup (removed duplicate files)
- Organized documentation in `docs/` folder
- Improved error handling
- Better logging and debugging
- Optimized LLM prompts and caching

### ğŸ“Š Project Organization
- **Cleaned Workspace**: Removed duplicate files and test scripts
- **Documentation**: All markdown files organized in `docs/` folder
- **Structure**: Clear separation of concerns
- **Deployment**: Systemd services for production deployment

---

**ğŸŒ Live Demo:** [https://ai4k8s.online](https://ai4k8s.online)

**ğŸ” Demo Credentials:** `admin` / `admin123`

**ğŸ“Š Status:** Production Ready âœ…

**ğŸš€ Last Updated:** January 2025

**ğŸ“ Infrastructure:** CrownLabs Kubernetes Cluster (Politecnico di Torino)
