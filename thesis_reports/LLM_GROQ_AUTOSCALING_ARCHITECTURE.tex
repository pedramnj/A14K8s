\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{float}

% Page geometry
\geometry{margin=1in}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={LLM-Powered Autoscaling Architecture Using Groq API},
    pdfauthor={Pedram Nikjooy}
}

% Code listing setup
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    breakatwhitespace=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    showstringspaces=false,
    tabsize=2,
    captionpos=b
}

% Python style
\lstdefinestyle{python}{
    language=Python,
    style=default,
    morekeywords={self,def,class,import,from,as,if,elif,else,for,while,return,None,True,False,try,except,finally,with,as}
}

% JSON style
\lstdefinestyle{json}{
    language=json,
    style=default
}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{LLM-Powered Autoscaling Architecture}
\fancyhead[R]{\thepage}
\fancyfoot[C]{Politecnico di Torino}

% Title formatting
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalsize\bfseries}{\thesubsubsection}{1em}{}

\begin{document}

% Title page
\begin{titlepage}
    \centering
    \vspace*{2cm}
    {\Huge\bfseries LLM-Powered Autoscaling Architecture\\Using Groq API\par}
    \vspace{1.5cm}
    {\Large\itshape Author: Pedram Nikjooy\par}
    \vspace{0.5cm}
    {\large Thesis: AI Agent for Kubernetes Management\par}
    \vspace{0.5cm}
    {\large Institution: Politecnico di Torino\par}
    \vspace{0.5cm}
    {\large Date: December 2025\par}
    \vfill
\end{titlepage}

\newpage
\tableofcontents
\newpage

\section{Executive Summary}

This document describes the architecture and implementation of AI4K8s's LLM-powered autoscaling system, which leverages Groq's high-performance LLM API to make intelligent, context-aware scaling decisions for Kubernetes deployments. The system combines machine learning forecasting with large language model reasoning to provide recommendations that consider performance, cost, stability, and predictive patterns.

\section{Introduction}

\subsection{Background}

Traditional Kubernetes autoscaling relies on reactive mechanisms (Horizontal Pod Autoscaler) that respond to current resource metrics. While effective, these systems lack the ability to:
\begin{itemize}
    \item Consider complex contextual factors simultaneously
    \item Provide human-readable reasoning for decisions
    \item Balance multiple competing objectives (cost vs. performance vs. stability)
    \item Learn from historical patterns and trends
\end{itemize}

\subsection{Solution: LLM-Powered Autoscaling}

AI4K8s integrates Groq's LLM API to provide intelligent autoscaling recommendations that:
\begin{itemize}
    \item Analyze multiple factors simultaneously (current metrics, forecasts, HPA status, patterns)
    \item Generate human-readable explanations for decisions
    \item Consider cost optimization, performance requirements, and stability
    \item Provide confidence scores and risk assessments
\end{itemize}

\section{Architecture Overview}

\subsection{System Components}

\begin{verbatim}
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€---â”€â”€â”
â”‚                    AI4K8s Web Application                     â”‚
â”‚  (Flask Web App - ai_kubernetes_web_app.py)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€--â”€â”€â”€â”
â”‚          Autoscaling Integration Layer                        â”‚
â”‚  (autoscaling_integration.py)                                 â”‚
â”‚  - Coordinates HPA, Predictive, Scheduled autoscaling         â”‚
â”‚  - Manages LLM advisor instance                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
        â–¼               â–¼               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HPA Manager  â”‚ â”‚ Predictive   â”‚ â”‚ Scheduled  â”‚
â”‚              â”‚ â”‚ Autoscaler   â”‚ â”‚ Autoscaler â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Predictive Monitoring System             â”‚
â”‚  (predictive_monitoring.py)                       â”‚
â”‚  - Collects metrics from Kubernetes               â”‚
â”‚  - Generates ML-based forecasts (6-hour horizon)  â”‚
â”‚  - Detects anomalies and trends                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LLM Autoscaling Advisor                        â”‚
â”‚  (llm_autoscaling_advisor.py)                           â”‚
â”‚  - Prepares context from metrics, forecasts, HPA status â”‚
â”‚  - Constructs prompts for Groq LLM                      â”‚
â”‚  - Parses and validates LLM responses                 â”‚
â”‚  - Implements caching and rate limiting                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Groq LLM API              â”‚
â”‚  - Model: llama-3.1-8b-instant (primary)    â”‚
â”‚  - Fallback: llama-3.1-70b-versatile         â”‚
â”‚  - Temperature: 0.3 (consistent decisions) â”‚
â”‚  - Max Tokens: 1000                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\end{verbatim}

\subsection{Data Flow}

\begin{enumerate}
    \item \textbf{Metric Collection}: Predictive Monitoring System collects CPU/Memory metrics from Kubernetes clusters
    \item \textbf{Forecast Generation}: ML models generate 6-hour ahead forecasts with trend analysis
    \item \textbf{Context Preparation}: LLM Advisor aggregates:
    \begin{itemize}
        \item Current resource usage (CPU\%, Memory\%, pod count)
        \item Forecast data (peak values, trends, predictions)
        \item HPA status (current/desired replicas, targets, scaling status)
        \item Deployment constraints (min/max replicas)
    \end{itemize}
    \item \textbf{LLM Analysis}: Groq LLM analyzes context and generates JSON recommendation
    \item \textbf{Response Parsing}: System extracts action, target replicas, reasoning, confidence
    \item \textbf{Caching}: Recommendation cached for 5 minutes to prevent excessive API calls
    \item \textbf{Display}: Web UI displays recommendation with reasoning and confidence score
\end{enumerate}

\section{Groq LLM Integration}

\subsection{Why Groq?}

\begin{itemize}
    \item \textbf{Performance}: Ultra-low latency inference (sub-second responses)
    \item \textbf{Cost}: Free tier with generous rate limits (14,400 requests/day)
    \item \textbf{Models}: Access to high-quality models (Llama 3.1 70B, 8B)
    \item \textbf{Reliability}: Production-ready API with fallback support
\end{itemize}

\subsection{Model Selection}

\textbf{Primary Model:} \texttt{llama-3.1-8b-instant}
\begin{itemize}
    \item Fast inference (sub-second responses)
    \item Lower latency
    \item Reliable and production-ready
    \item \textbf{Note:} The 70B model (\texttt{llama-3.1-70b-versatile}) was decommissioned by Groq, so 8B is now the primary model
\end{itemize}

\textbf{Fallback Model:} \texttt{llama-3.1-70b-versatile}
\begin{itemize}
    \item Kept as fallback for compatibility
    \item May not be available (decommissioned)
\end{itemize}

\textbf{Implementation Location:} \texttt{llm\_autoscaling\_advisor.py} lines 38-42

\begin{lstlisting}[style=python, caption={Model Configuration}]
self.model = "llama-3.1-8b-instant"  # Primary model (fast, reliable)
self.fallback_model = "llama-3.1-70b-versatile"  # Fallback
\end{lstlisting}

\subsection{API Configuration}

\begin{lstlisting}[style=python, caption={Groq API Call Configuration}]
response = self.client.chat.completions.create(
    model=self.model,  # llama-3.1-8b-instant
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ],
    temperature=0.3,      # Low temperature for consistent decisions
    max_tokens=1000       # Sufficient for detailed reasoning
)
\end{lstlisting}

\textbf{Temperature Setting (0.3):}
\begin{itemize}
    \item Lower temperature = more deterministic, consistent outputs
    \item Important for autoscaling where consistency is critical
    \item Prevents random variations in recommendations
\end{itemize}

\section{Prompt Engineering}

\subsection{System Prompt}

The system prompt establishes the LLM's role and expertise. It is defined in \texttt{llm\_autoscaling\_advisor.py} in the \texttt{\_create\_system\_prompt()} method (lines 378-406).

\textbf{File:} \texttt{llm\_autoscaling\_advisor.py}\\
\textbf{Method:} \texttt{LLMAutoscalingAdvisor.\_create\_system\_prompt()}\\
\textbf{Called from:} \texttt{analyze\_scaling\_decision()} method (line 183)

\textbf{System Prompt Content:}
\begin{verbatim}
You are an expert Kubernetes autoscaling advisor with deep knowledge of:
- Resource optimization and cost management
- Performance requirements and SLA considerations
- Scaling best practices and anti-patterns
- Predictive analysis and trend interpretation

Your role is to analyze deployment metrics, forecasts, and patterns to make 
intelligent scaling recommendations.

Consider these factors:
1. **Performance**: Ensure adequate resources to meet performance requirements
2. **Cost**: Minimize resource usage while maintaining performance
3. **Stability**: Avoid rapid scaling changes that could cause instability
4. **Predictions**: Use forecast data to proactively scale before demand arrives
5. **Constraints**: Respect min/max replica limits and current cluster capacity

Respond in JSON format with:
{
  "action": "scale_up" | "scale_down" | "maintain" | "at_max",
  "target_replicas": <number>,
  "confidence": <0.0-1.0>,
  "reasoning": "<detailed explanation>",
  "factors_considered": ["factor1", "factor2", ...],
  "risk_assessment": "low" | "medium" | "high",
  "cost_impact": "low" | "medium" | "high",
  "performance_impact": "positive" | "neutral" | "negative",
  "recommended_timing": "immediate" | "gradual" | "scheduled"
}
\end{verbatim}

\textbf{Code Implementation:}
\begin{lstlisting}[style=python, caption={System Prompt Creation}]
# File: llm_autoscaling_advisor.py, lines 378-406
def _create_system_prompt(self) -> str:
    """Create system prompt for LLM"""
    return """You are an expert Kubernetes autoscaling advisor...
    [prompt content as shown above]
    """
\end{lstlisting}

\subsection{User Prompt Structure}

The user prompt is dynamically generated with deployment-specific context. It is defined in \texttt{llm\_autoscaling\_advisor.py} in the \texttt{\_create\_user\_prompt()} method (lines 408-457).

\textbf{File:} \texttt{llm\_autoscaling\_advisor.py}\\
\textbf{Method:} \texttt{LLMAutoscalingAdvisor.\_create\_user\_prompt(context: Dict[str, Any])}\\
\textbf{Called from:} \texttt{analyze\_scaling\_decision()} method (line 186)\\
\textbf{Input:} Context dictionary containing deployment metrics, forecasts, and HPA status

\textbf{User Prompt Template:}
\begin{verbatim}
Analyze the following Kubernetes deployment autoscaling scenario and provide a recommendation:

**Deployment Information:**
- Name: {deployment_name}
- Namespace: {namespace}
- Current Replicas: {current_replicas}
- Min Replicas: {min_replicas}
- Max Replicas: {max_replicas}

**Current Resource Usage:**
- CPU: {cpu_usage}%
- Memory: {memory_usage}%
- Running Pods: {running_pods}/{total_pods}

**Forecast Data:**
- CPU Current: {current}%, Peak: {peak}%, Trend: {trend}
- Memory Current: {current}%, Peak: {peak}%, Trend: {trend}
- CPU Predictions (next 6 hours): {predictions}
- Memory Predictions (next 6 hours): {predictions}

**HPA Status:**
- HPA Active: {yes/no}
- Current Replicas: {replicas}
- Desired Replicas: {desired}
- Target CPU: {target}%
- Target Memory: {target}%
- Scaling Status: {status}

**Analysis Request:**
Based on the above information, provide an intelligent scaling recommendation 
considering:
1. Current resource pressure (CPU/Memory usage)
2. Predicted future demand (forecast trends)
3. Cost optimization opportunities
4. Performance requirements
5. Stability and risk factors

Provide your recommendation in the specified JSON format.
\end{verbatim}

\textbf{Code Implementation:}
\begin{lstlisting}[style=python, caption={User Prompt Creation}]
# File: llm_autoscaling_advisor.py, lines 408-457
def _create_user_prompt(self, context: Dict[str, Any]) -> str:
    """Create user prompt with context"""
    prompt = f"""Analyze the following Kubernetes deployment...
    **Deployment Information:**
    - Name: {context['deployment']['name']}
    - Namespace: {context['deployment']['namespace']}
    ...
    """
    # Dynamically adds HPA status if available
    if context['hpa']['exists']:
        prompt += f"- HPA Active: Yes\n..."
    else:
        prompt += "- HPA Active: No (reactive scaling not configured)\n"
    
    return prompt
\end{lstlisting}

\subsection{Prompt Usage Flow}

\textbf{Complete Flow:}

\begin{enumerate}
    \item \textbf{Context Preparation} (\texttt{llm\_autoscaling\_advisor.py}, lines 174-180):
    \begin{lstlisting}[style=python]
context = self._prepare_context(
    deployment_name, namespace, current_metrics, forecast,
    hpa_status, historical_patterns, current_replicas,
    min_replicas, max_replicas
)
    \end{lstlisting}
    
    \item \textbf{Prompt Creation} (\texttt{llm\_autoscaling\_advisor.py}, lines 183-186):
    \begin{lstlisting}[style=python]
system_prompt = self._create_system_prompt()
user_prompt = self._create_user_prompt(context)
    \end{lstlisting}
    
    \item \textbf{API Call} (\texttt{llm\_autoscaling\_advisor.py}, lines 191-199):
    \begin{lstlisting}[style=python]
response = self.client.chat.completions.create(
    model=self.model,
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ],
    temperature=0.3,
    max_tokens=1000
)
    \end{lstlisting}
\end{enumerate}

\textbf{Example Real-World Prompt:}

For a deployment with:
\begin{itemize}
    \item Name: \texttt{test-app-autoscaling}
    \item Namespace: \texttt{ai4k8s-test}
    \item Current Replicas: 10
    \item CPU Usage: 169.2\%
    \item Memory Usage: 11.7\%
    \item Forecast: CPU predictions [0, 0, 0, 0, 0, 0], Memory predictions [0, 0, 0, 0, 0, 0]
\end{itemize}

The generated user prompt would be:
\begin{verbatim}
Analyze the following Kubernetes deployment autoscaling scenario and provide a recommendation:

**Deployment Information:**
- Name: test-app-autoscaling
- Namespace: ai4k8s-test
- Current Replicas: 10
- Min Replicas: 2
- Max Replicas: 10

**Current Resource Usage:**
- CPU: 169.2%
- Memory: 11.7%
- Running Pods: 10/10

**Forecast Data:**
- CPU Current: 169.2%, Peak: 169.2%, Trend: stable
- Memory Current: 11.7%, Peak: 11.7%, Trend: stable
- CPU Predictions (next 6 hours): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
- Memory Predictions (next 6 hours): [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

**HPA Status:**
- HPA Active: Yes
- Current Replicas: 10
- Desired Replicas: 10
- Target CPU: 70%
- Target Memory: 80%
- Scaling Status: stable

**Analysis Request:**
Based on the above information, provide an intelligent scaling recommendation...
\end{verbatim}

\subsection{Response Format}

The LLM is instructed to respond in JSON format. The response is parsed in \texttt{llm\_autoscaling\_advisor.py} in the \texttt{\_parse\_llm\_response()} method (lines 272-300).

\textbf{File:} \texttt{llm\_autoscaling\_advisor.py}\\
\textbf{Method:} \texttt{LLMAutoscalingAdvisor.\_parse\_llm\_response(llm\_output: str)}\\
\textbf{Called from:} \texttt{analyze\_scaling\_decision()} method (line 222)

\textbf{Expected JSON Format:}
\begin{lstlisting}[style=json, caption={Expected LLM Response Format}]
{
  "action": "scale_up" | "scale_down" | "maintain" | "at_max",
  "target_replicas": <number>,
  "confidence": <0.0-1.0>,
  "reasoning": "<detailed explanation>",
  "factors_considered": ["factor1", "factor2", ...],
  "risk_assessment": "low" | "medium" | "high",
  "cost_impact": "low" | "medium" | "high",
  "performance_impact": "positive" | "neutral" | "negative",
  "recommended_timing": "immediate" | "gradual" | "scheduled"
}
\end{lstlisting}

\textbf{Example Real Response:}
\begin{lstlisting}[style=json, caption={Example LLM Response}]
{
  "action": "scale_down",
  "target_replicas": 8,
  "confidence": 0.8,
  "reasoning": "Based on the current resource usage (CPU: 169.2%, Memory: 11.7%) and predicted future demand (CPU/Memory predictions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), it is likely that the current replica count of 10 is more than sufficient. The forecast indicates no significant increase in demand over the next 6 hours, and scaling down to 8 replicas will reduce costs while maintaining performance.",
  "factors_considered": [
    "Current resource pressure",
    "Predicted future demand",
    "Cost optimization opportunities",
    "Performance requirements",
    "Stability factors"
  ],
  "risk_assessment": "low",
  "cost_impact": "low",
  "performance_impact": "neutral",
  "recommended_timing": "immediate"
}
\end{lstlisting}

\section{Caching and Rate Limiting}

\subsection{Cache Strategy}

\textbf{Purpose:} Prevent excessive API calls and stabilize recommendations

\textbf{Cache Key Generation:}
\begin{itemize}
    \item Aggressive rounding to create stable cache keys:
    \begin{itemize}
        \item CPU: Round to nearest 25\% (e.g., 173\% $\rightarrow$ 175\%, 180\% $\rightarrow$ 175\%)
        \item Memory: Round to nearest 5\% (e.g., 11.7\% $\rightarrow$ 10\%, 12.3\% $\rightarrow$ 10\%)
        \item Replicas: Use as-is (discrete value)
        \item Forecast peaks: Round similarly
        \item Trends: Include trend direction (increasing/decreasing/stable)
    \end{itemize}
\end{itemize}

\textbf{Cache TTL:} 5 minutes (300 seconds)

\textbf{Example Cache Key:}
\begin{lstlisting}[style=python, caption={Cache Key Example}]
{
    'deployment': 'default/my-app',
    'replicas': 10,
    'cpu': 175,        # Rounded from 173%
    'memory': 10,      # Rounded from 11.7%
    'cpu_peak': 200,   # Rounded forecast peak
    'memory_peak': 15, # Rounded forecast peak
    'cpu_trend': 'increasing',
    'memory_trend': 'stable'
}
\end{lstlisting}

\subsection{Rate Limiting}

\textbf{Minimum Interval:} 5 minutes between LLM calls for the same deployment

\textbf{Rationale:}
\begin{itemize}
    \item Prevents API quota exhaustion
    \item Reduces costs
    \item Ensures recommendations remain stable
    \item Respects Groq free tier limits (14,400 requests/day)
\end{itemize}

\textbf{Implementation:}
\begin{lstlisting}[style=python, caption={Rate Limiting Implementation}]
self.min_llm_interval = 300  # 5 minutes
if time_since_last_call < self.min_llm_interval:
    return {
        'success': False,
        'error': 'Rate-limited',
        'rate_limited': True
    }
\end{lstlisting}

\subsection{Cache Management}

\begin{itemize}
    \item \textbf{Size Limit:} Maximum 100 cached recommendations
    \item \textbf{Eviction:} LRU (Least Recently Used) when limit reached
    \item \textbf{Cleanup:} Old entries removed automatically
\end{itemize}

\section{Response Parsing and Validation}

\subsection{Parsing Strategy}

The system handles multiple response formats:

\begin{enumerate}
    \item \textbf{Direct JSON:} \texttt{\{"action": "scale\_up", ...\}}
    \item \textbf{Markdown Code Block:} \texttt{```json \{...\} ```}
    \item \textbf{Embedded JSON:} Extract JSON object from text
    \item \textbf{Fallback:} Parse text to extract action and replicas
\end{enumerate}

\subsection{Validation}

\begin{itemize}
    \item \textbf{Action:} Must be one of: \texttt{scale\_up}, \texttt{scale\_down}, \texttt{maintain}, \texttt{at\_max}
    \item \textbf{Target Replicas:} Must be within \texttt{[min\_replicas, max\_replicas]}
    \item \textbf{Confidence:} Must be between 0.0 and 1.0
    \item \textbf{Reasoning:} Required, minimum 50 characters
\end{itemize}

\subsection{Error Handling}

\begin{itemize}
    \item \textbf{API Failures:} Fallback to ML-based recommendations
    \item \textbf{Parse Errors:} Attempt fallback parsing, log warning
    \item \textbf{Invalid Responses:} Return error with raw response for debugging
\end{itemize}

\section{Integration with Predictive Autoscaling}

\subsection{Workflow}

\begin{enumerate}
    \item \textbf{User Enables Predictive Autoscaling:}
    \begin{itemize}
        \item User selects deployment and sets min/max replicas
        \item System marks deployment with Kubernetes annotations
    \end{itemize}
    
    \item \textbf{Metric Collection:}
    \begin{itemize}
        \item System collects deployment-specific metrics
        \item Calculates CPU/Memory usage percentages
    \end{itemize}
    
    \item \textbf{Forecast Generation:}
    \begin{itemize}
        \item ML models generate 6-hour forecasts
        \item Identifies trends (increasing/decreasing/stable)
    \end{itemize}
    
    \item \textbf{LLM Analysis:}
    \begin{itemize}
        \item LLM Advisor prepares context
        \item Calls Groq API with structured prompt
        \item Receives JSON recommendation
    \end{itemize}
    
    \item \textbf{Recommendation Display:}
    \begin{itemize}
        \item Web UI shows:
        \begin{itemize}
            \item Action (Scale Up/Down/Maintain/At Max)
            \item Target Replicas
            \item Confidence Score
            \item Detailed Reasoning
            \item Risk Assessment
            \item Cost/Performance Impact
        \end{itemize}
    \end{itemize}
\end{enumerate}

\subsection{Code Integration}

\textbf{File:} \texttt{predictive\_autoscaler.py}\\
\textbf{Class:} \texttt{PredictiveAutoscaler}\\
\textbf{Method:} \texttt{get\_scaling\_recommendation()} (lines 631-778)

\textbf{Initialization} (lines 42-55):
\begin{lstlisting}[style=python, caption={LLM Advisor Initialization}]
# Initialize LLM advisor
self.use_llm = use_llm
self.llm_advisor = None
if use_llm:
    try:
        self.llm_advisor = LLMAutoscalingAdvisor()
        if self.llm_advisor.client:
            logger.info("âœ… LLM autoscaling advisor enabled")
        else:
            logger.warning("âš ï¸  LLM advisor requested but no API key available")
            self.use_llm = False
    except Exception as e:
        logger.warning(f"âš ï¸  Failed to initialize LLM advisor: {e}")
        self.use_llm = False
\end{lstlisting}

\textbf{LLM Recommendation Call} (lines 680-737):
\begin{lstlisting}[style=python, caption={LLM Recommendation Integration}, basicstyle=\ttfamily\tiny]
# Try LLM-based recommendation first
llm_recommendation = None
if self.use_llm and self.llm_advisor:
    try:
        # Get HPA status if exists
        hpa_name = f"{deployment_name}-hpa"
        hpa_status = None
        hpa_result = self.hpa_manager.get_hpa(hpa_name, namespace)
        if hpa_result.get('success'):
            hpa_status = hpa_result.get('status', {})
        
        # Prepare metrics
        current_metrics = {
            'cpu_usage': cpu_current,
            'memory_usage': memory_current,
            'pod_count': current_replicas,
            'running_pod_count': current_replicas
        }
        
        # Prepare forecast
        forecast_data = {
            'cpu': {
                'current': cpu_forecast.current_value,
                'peak': max_predicted_cpu,
                'trend': cpu_forecast.trend,
                'predictions': cpu_forecast.predicted_values
            },
            'memory': {
                'current': memory_forecast.current_value,
                'peak': max_predicted_memory,
                'trend': memory_forecast.trend,
                'predictions': memory_forecast.predicted_values
            }
        }
        
        # Get LLM recommendation
        llm_result = self.llm_advisor.get_intelligent_recommendation(
            deployment_name=deployment_name,
            namespace=namespace,
            current_metrics=current_metrics,
            forecast=forecast_data,
            hpa_status=hpa_status,
            current_replicas=current_replicas,
            min_replicas=min_replicas,
            max_replicas=max_replicas
        )
        
        if llm_result.get('success'):
            llm_recommendation = llm_result.get('recommendation', {})
            logger.info(f"âœ… LLM recommendation: {llm_recommendation.get('action')} -> {llm_recommendation.get('target_replicas')} replicas")
        elif llm_result.get('rate_limited'):
            logger.info(f"â¸ï¸  LLM rate-limited, using fallback recommendation")
            llm_recommendation = None
        else:
            logger.warning(f"âš ï¸  LLM recommendation failed: {llm_result.get('error')}, using fallback")
    except Exception as e:
        logger.warning(f"âš ï¸  Error getting LLM recommendation: {e}, using fallback")

# Use LLM recommendation if available, otherwise fallback to rule-based
if llm_recommendation:
    action = {
        'action': llm_recommendation.get('action', 'none'),
        'target_replicas': llm_recommendation.get('target_replicas', current_replicas),
        'reason': llm_recommendation.get('reasoning', 'LLM-based recommendation'),
        'confidence': llm_recommendation.get('confidence', 0.5),
        'llm_recommendation': llm_recommendation
    }
else:
    # Fallback to rule-based recommendation
    action = self._determine_scaling_action(...)
\end{lstlisting}

\textbf{Integration with Web API} (\texttt{ai\_kubernetes\_web\_app.py}, lines 2133-2157):
\begin{lstlisting}[style=python, caption={Web API Endpoint}]
@app.route('/api/autoscaling/recommendations/<int:server_id>')
def get_scaling_recommendations(server_id):
    """Get scaling recommendations"""
    server = Server.query.filter_by(id=server_id, user_id=session['user_id']).first_or_404()
    deployment_name = request.args.get('deployment')
    namespace = request.args.get('namespace', 'default')
    
    autoscaling = get_autoscaling_instance(server_id)
    result = autoscaling.get_scaling_recommendations(deployment_name, namespace)
    return jsonify(result)
\end{lstlisting}

\textbf{Integration Layer} (\texttt{autoscaling\_integration.py}, lines 166-259):
\begin{lstlisting}[style=python, caption={Integration Layer Method}]
def get_scaling_recommendations(self, deployment_name: str, namespace: str = "default"):
    """Get scaling recommendations from all sources including LLM"""
    recommendations = {
        'predictive': None,
        'reactive': None,
        'scheduled': None,
        'llm': None
    }
    
    # Predictive recommendation (may include LLM if enabled)
    pred_rec = self.predictive_autoscaler.get_scaling_recommendation(
        deployment_name, namespace
    )
    if pred_rec['success']:
        recommendations['predictive'] = pred_rec
        
        # Extract LLM recommendation if present
        if pred_rec.get('llm_used') and pred_rec.get('recommendation', {}).get('llm_recommendation'):
            recommendations['llm'] = {
                'success': True,
                'recommendation': pred_rec['recommendation']['llm_recommendation'],
                'source': 'predictive_with_llm'
            }
\end{lstlisting}

\section{User Interface Integration}

\subsection{Display Format}

The web UI displays LLM recommendations with:

\begin{verbatim}
ğŸ¤– LLM-Powered Recommendation: Scale Down

80% Confidence

Reasoning:
Based on the current resource usage and forecast data, it appears that 
the deployment is currently underutilized. The CPU and memory usage are 
both below 20%, indicating that there is ample capacity to handle the 
current workload...

Target Replicas: 5

Factors Considered:
- Current resource pressure
- Predicted future demand
- Cost optimization opportunities
- Performance requirements
- Stability factors

Risk Assessment: LOW
Cost Impact: LOW (reduction)
Performance Impact: NEUTRAL
Recommended Timing: IMMEDIATE
\end{verbatim}

\subsection{Visual Indicators}

\begin{itemize}
    \item \textbf{Color Coding:}
    \begin{itemize}
        \item Green: Scale Down (cost optimization)
        \item Blue: Maintain (stable)
        \item Orange: Scale Up (performance)
        \item Red: At Max (capacity limit)
    \end{itemize}
    \item \textbf{Confidence Badge:} Visual indicator (0-100\%)
\end{itemize}

\section{Benefits and Advantages}

\subsection{Multi-Factor Analysis}

Unlike rule-based systems, LLM can simultaneously consider:
\begin{itemize}
    \item Current resource pressure
    \item Predicted future demand
    \item Cost implications
    \item Performance requirements
    \item Stability concerns
    \item Historical patterns
\end{itemize}

\subsection{Human-Readable Reasoning}

Every recommendation includes:
\begin{itemize}
    \item Detailed explanation of decision
    \item Factors considered
    \item Risk assessment
    \item Expected impact
\end{itemize}

\subsection{Contextual Awareness}

LLM understands:
\begin{itemize}
    \item Relationships between metrics
    \item Temporal patterns (trends)
    \item Trade-offs between objectives
    \item Best practices and anti-patterns
\end{itemize}

\subsection{Cost Optimization}

LLM can identify:
\begin{itemize}
    \item Over-provisioned resources
    \item Underutilized capacity
    \item Optimal scaling points
    \item Cost-performance trade-offs
\end{itemize}

\section{Performance Characteristics}

\subsection{Latency}

\begin{itemize}
    \item \textbf{LLM API Call:} $\sim$500-1500ms (Groq)
    \item \textbf{Total Recommendation Time:} $\sim$1-2 seconds
    \item \textbf{Cache Hit:} $<$10ms
\end{itemize}

\subsection{Throughput}

\begin{itemize}
    \item \textbf{Free Tier Limit:} 14,400 requests/day
    \item \textbf{With Caching:} Effectively unlimited (cache hit rate $\sim$80-90\%)
    \item \textbf{Rate Limiting:} 1 call per deployment per 5 minutes
\end{itemize}

\subsection{Accuracy}

\begin{itemize}
    \item \textbf{Confidence Scores:} Typically 70-90\%
    \item \textbf{Action Accuracy:} Validated against ML-based recommendations
    \item \textbf{Replica Prediction:} Within $\pm$1 replica of optimal
\end{itemize}

\section{Limitations and Considerations}

\subsection{API Dependencies}

\begin{itemize}
    \item \textbf{Requires Internet:} Groq API must be accessible
    \item \textbf{API Key Management:} Secure storage required
    \item \textbf{Rate Limits:} Free tier has daily limits
\end{itemize}

\subsection{Cost Considerations}

\begin{itemize}
    \item \textbf{Free Tier:} Sufficient for most use cases
    \item \textbf{Paid Tier:} May be needed for high-volume deployments
    \item \textbf{Caching:} Reduces API calls significantly
\end{itemize}

\subsection{Consistency}

\begin{itemize}
    \item \textbf{Temperature Setting:} Low (0.3) for consistency
    \item \textbf{Cache:} Prevents rapid changes
    \item \textbf{Rate Limiting:} Ensures stable recommendations
\end{itemize}

\subsection{Fallback Mechanisms}

\begin{itemize}
    \item \textbf{ML-Based Recommendations:} Always available as fallback
    \item \textbf{Rule-Based Scaling:} HPA continues to function independently
    \item \textbf{Graceful Degradation:} System continues operating if LLM unavailable
\end{itemize}

\section{Future Enhancements}

\subsection{Planned Improvements}

\begin{enumerate}
    \item \textbf{Historical Pattern Learning:}
    \begin{itemize}
        \item Feed historical scaling decisions to LLM
        \item Learn from past successes/failures
    \end{itemize}
    
    \item \textbf{Multi-Deployment Analysis:}
    \begin{itemize}
        \item Consider cluster-wide resource constraints
        \item Optimize across multiple deployments
    \end{itemize}
    
    \item \textbf{Custom Prompts:}
    \begin{itemize}
        \item Allow users to customize LLM behavior
        \item Industry-specific optimizations
    \end{itemize}
    
    \item \textbf{A/B Testing:}
    \begin{itemize}
        \item Compare LLM vs. ML recommendations
        \item Measure actual performance improvements
    \end{itemize}
    
    \item \textbf{Fine-Tuning:}
    \begin{itemize}
        \item Fine-tune models on Kubernetes-specific data
        \item Improve accuracy for specific use cases
    \end{itemize}
\end{enumerate}

\section{Conclusion}

The integration of Groq LLM API into AI4K8s's autoscaling system represents a significant advancement in intelligent Kubernetes resource management. By combining machine learning forecasting with large language model reasoning, the system provides:

\begin{itemize}
    \item \textbf{Intelligent Recommendations:} Multi-factor analysis beyond simple thresholds
    \item \textbf{Human-Readable Explanations:} Transparent decision-making process
    \item \textbf{Cost Optimization:} Intelligent resource allocation
    \item \textbf{Stability:} Caching and rate limiting prevent rapid changes
    \item \textbf{Scalability:} Efficient API usage with caching
\end{itemize}

The architecture demonstrates how modern LLM APIs can enhance traditional infrastructure management systems, providing both automation and explainability.

\section{References}

\begin{itemize}
    \item \textbf{Groq API Documentation:} \url{https://console.groq.com/docs}
    \item \textbf{Kubernetes HPA:} \url{https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/}
    \item \textbf{Llama 3.1 Models:} \url{https://llama.meta.com/llama3.1/}
    \item \textbf{AI4K8s Repository:} \url{https://github.com/pedramnj/ai4k8s}
\end{itemize}

\appendix

\section{Code Structure}

\subsection{Key Files and Their Roles}

\begin{enumerate}
    \item \textbf{\texttt{llm\_autoscaling\_advisor.py}} (517 lines)
    \begin{itemize}
        \item \textbf{Purpose:} Core LLM advisor implementation
        \item \textbf{Key Classes:} \texttt{LLMAutoscalingAdvisor}
        \item \textbf{Key Methods:}
        \begin{itemize}
            \item \texttt{\_\_init\_\_()} (lines 30-59): Initialize Groq client and configure models
            \item \texttt{analyze\_scaling\_decision()} (lines 102-270): Main entry point for LLM analysis
            \item \texttt{\_create\_system\_prompt()} (lines 378-406): Generate system prompt
            \item \texttt{\_create\_user\_prompt()} (lines 408-457): Generate user prompt with context
            \item \texttt{\_parse\_llm\_response()} (lines 272-300): Parse and validate LLM JSON response
            \item \texttt{\_get\_cache\_key()} (lines 61-100): Generate cache keys with aggressive rounding
            \item \texttt{get\_intelligent\_recommendation()} (lines 460-482): Convenience wrapper method
        \end{itemize}
    \end{itemize}
    
    \item \textbf{\texttt{predictive\_autoscaler.py}} (787 lines)
    \begin{itemize}
        \item \textbf{Purpose:} Predictive autoscaling with LLM support
        \item \textbf{Key Classes:} \texttt{PredictiveAutoscaler}
        \item \textbf{Key Methods:}
        \begin{itemize}
            \item \texttt{\_\_init\_\_()} (lines 32-60): Initialize with LLM advisor
            \item \texttt{get\_scaling\_recommendation()} (lines 631-778): Get recommendation (LLM or fallback)
            \item \texttt{predict\_and\_scale()} (lines 62-319): Execute predictive scaling
            \item \texttt{\_get\_deployment\_metrics()} (lines 370-450): Collect deployment-specific metrics
        \end{itemize}
    \end{itemize}
    
    \item \textbf{\texttt{autoscaling\_integration.py}} (274 lines)
    \begin{itemize}
        \item \textbf{Purpose:} Main integration layer coordinating all autoscaling components
        \item \textbf{Key Classes:} \texttt{AutoscalingIntegration}
        \item \textbf{Key Methods:}
        \begin{itemize}
            \item \texttt{\_\_init\_\_()} (lines 28-54): Initialize all components including LLM advisor
            \item \texttt{get\_scaling\_recommendations()} (lines 166-259): Aggregate recommendations from all sources
            \item \texttt{enable\_predictive\_autoscaling()} (lines 124-148): Enable predictive autoscaling for deployment
        \end{itemize}
    \end{itemize}
    
    \item \textbf{\texttt{ai\_kubernetes\_web\_app.py}} (2600+ lines)
    \begin{itemize}
        \item \textbf{Purpose:} Flask web application with API endpoints
        \item \textbf{Key Routes:}
        \begin{itemize}
            \item \texttt{/api/autoscaling/recommendations/<server\_id>} (lines 2133-2157): Get scaling recommendations API
            \item \texttt{/api/autoscaling/predictive/enable/<server\_id>} (lines 2070-2095): Enable predictive autoscaling API
            \item \texttt{/autoscaling/<int:server\_id>} (lines 1950-1965): Autoscaling dashboard page
        \end{itemize}
    \end{itemize}
    
    \item \textbf{\texttt{templates/autoscaling.html}} (1054 lines)
    \begin{itemize}
        \item \textbf{Purpose:} Web UI for displaying autoscaling recommendations
        \item \textbf{Key Functions:}
        \begin{itemize}
            \item \texttt{loadPredictiveForecast()} (lines 620-732): Load and display LLM recommendations
            \item \texttt{displayRecommendations()} (lines 735-888): Render recommendation cards with LLM badge
            \item \texttt{refreshAutoscaling()} (lines 322-470): Refresh autoscaling status and recommendations
        \end{itemize}
    \end{itemize}
\end{enumerate}

\subsection{Key Classes and Methods}

\textbf{\texttt{LLMAutoscalingAdvisor}} (\texttt{llm\_autoscaling\_advisor.py})
\begin{itemize}
    \item \textbf{Purpose:} Manages all LLM interactions with Groq API
    \item \textbf{Initialization:} Requires \texttt{GROQ\_API\_KEY} environment variable
    \item \textbf{Main Method:} \texttt{analyze\_scaling\_decision()} - Orchestrates prompt creation, API call, and response parsing
\end{itemize}

\textbf{\texttt{PredictiveAutoscaler}} (\texttt{predictive\_autoscaler.py})
\begin{itemize}
    \item \textbf{Purpose:} Combines ML forecasting with LLM reasoning
    \item \textbf{LLM Integration:} Calls \texttt{LLMAutoscalingAdvisor} when \texttt{use\_llm=True}
    \item \textbf{Fallback:} Uses rule-based scaling if LLM unavailable or fails
\end{itemize}

\textbf{\texttt{AutoscalingIntegration}} (\texttt{autoscaling\_integration.py})
\begin{itemize}
    \item \textbf{Purpose:} Coordinates HPA, Predictive, and Scheduled autoscaling
    \item \textbf{LLM Integration:} Initializes standalone \texttt{LLMAutoscalingAdvisor} instance
    \item \textbf{API:} Provides unified interface for web application
\end{itemize}

\subsection{Code Flow Diagram}

\begin{verbatim}
Web Request
    â†“
/api/autoscaling/recommendations/<server_id>
    â†“
AutoscalingIntegration.get_scaling_recommendations()
    â†“
PredictiveAutoscaler.get_scaling_recommendation()
    â†“
LLMAutoscalingAdvisor.analyze_scaling_decision()
    â†“
    â”œâ”€â†’ _prepare_context()          [Prepare metrics, forecasts, HPA]
    â”œâ”€â†’ _create_system_prompt()     [Generate system prompt]
    â”œâ”€â†’ _create_user_prompt()       [Generate user prompt with context]
    â”œâ”€â†’ Groq API Call               [POST to api.groq.com/openai/v1/chat/completions]
    â”œâ”€â†’ _parse_llm_response()      [Parse JSON response]
    â””â”€â†’ Cache Result                [Store in recommendation_cache]
    â†“
Return Recommendation
    â†“
Web UI Display (autoscaling.html)
\end{verbatim}

\section{Complete End-to-End Flow Example}

\subsection{User Action to LLM Recommendation}

\textbf{Step 1: User Enables Predictive Autoscaling}
\begin{itemize}
    \item \textbf{File:} \texttt{templates/autoscaling.html} (lines 563-596)
    \item \textbf{Function:} \texttt{enablePredictiveAutoscaling(event)}
    \item \textbf{Action:} User fills form and clicks "Enable Predictive Autoscaling"
    \item \textbf{API Call:} \texttt{POST /api/autoscaling/predictive/enable/<server\_id>}
\end{itemize}

\textbf{Step 2: Backend Receives Request}
\begin{itemize}
    \item \textbf{File:} \texttt{ai\_kubernetes\_web\_app.py} (lines 2070-2095)
    \item \textbf{Route:} \texttt{@app.route('/api/autoscaling/predictive/enable/<int:server\_id>')}
    \item \textbf{Handler:} Calls \texttt{autoscaling.enable\_predictive\_autoscaling()}
\end{itemize}

\textbf{Step 3: Predictive Autoscaler Initializes}
\begin{itemize}
    \item \textbf{File:} \texttt{autoscaling\_integration.py} (lines 124-148)
    \item \textbf{Method:} \texttt{enable\_predictive\_autoscaling()}
    \item \textbf{Action:} Calls \texttt{predictive\_autoscaler.predict\_and\_scale()}
\end{itemize}

\textbf{Step 4: Get Scaling Recommendation}
\begin{itemize}
    \item \textbf{File:} \texttt{predictive\_autoscaler.py} (lines 631-778)
    \item \textbf{Method:} \texttt{get\_scaling\_recommendation()}
    \item \textbf{Action:}
    \begin{enumerate}
        \item Collects deployment metrics (lines 644-646)
        \item Generates forecasts (lines 652-657)
        \item Calls LLM advisor (lines 716-725)
    \end{enumerate}
\end{itemize}

\textbf{Step 5: LLM Advisor Processes Request}
\begin{itemize}
    \item \textbf{File:} \texttt{llm\_autoscaling\_advisor.py} (lines 102-270)
    \item \textbf{Method:} \texttt{analyze\_scaling\_decision()}
    \item \textbf{Actions:}
    \begin{enumerate}
        \item Checks cache (lines 138-159)
        \item Checks rate limit (lines 161-172)
        \item Prepares context (lines 176-180)
        \item Creates prompts (lines 183-186)
        \item Calls Groq API (lines 191-199)
        \item Parses response (lines 218-222)
        \item Caches result (lines 234-260)
    \end{enumerate}
\end{itemize}

\textbf{Step 6: Response Returns to Web UI}
\begin{itemize}
    \item \textbf{File:} \texttt{templates/autoscaling.html} (lines 620-732)
    \item \textbf{Function:} \texttt{loadPredictiveForecast()}
    \item \textbf{Action:} Displays recommendation with "ğŸ¤– LLM-Powered Recommendation" badge
\end{itemize}

\subsection{Complete Code Trace}

\begin{verbatim}
User clicks "Enable Predictive Autoscaling"
    â†“
[autoscaling.html:563] enablePredictiveAutoscaling()
    â†“
POST /api/autoscaling/predictive/enable/1
    â†“
[ai_kubernetes_web_app.py:2070] enable_predictive_autoscaling()
    â†“
[autoscaling_integration.py:124] enable_predictive_autoscaling()
    â†“
[predictive_autoscaler.py:631] get_scaling_recommendation()
    â†“
[predictive_autoscaler.py:716] llm_advisor.get_intelligent_recommendation()
    â†“
[llm_autoscaling_advisor.py:102] analyze_scaling_decision()
    â†“
[llm_autoscaling_advisor.py:176] _prepare_context()
    â†“
[llm_autoscaling_advisor.py:183] _create_system_prompt()
    â†“
[llm_autoscaling_advisor.py:186] _create_user_prompt()
    â†“
[llm_autoscaling_advisor.py:191] client.chat.completions.create()
    â†“ HTTP POST to api.groq.com
    â†“
[llm_autoscaling_advisor.py:222] _parse_llm_response()
    â†“
[llm_autoscaling_advisor.py:234] Cache result
    â†“
Return recommendation
    â†“
[predictive_autoscaler.py:728] Store llm_recommendation
    â†“
[autoscaling_integration.py:185] Extract LLM recommendation
    â†“
[ai_kubernetes_web_app.py:2151] Return JSON response
    â†“
[autoscaling.html:696] fetch('/api/autoscaling/recommendations/...')
    â†“
[autoscaling.html:735] displayRecommendations()
    â†“
Display "ğŸ¤– LLM-Powered Recommendation" in UI
\end{verbatim}

\subsection{File-by-File Breakdown}

\begin{table}[H]
\centering
\small
\begin{tabular}{@{}p{1cm}p{4cm}p{2cm}p{7cm}@{}}
\toprule
Step & File & Lines & What Happens \\
\midrule
1 & \texttt{templates/autoscaling.html} & 563-596 & User submits form, JavaScript calls API \\
2 & \texttt{ai\_kubernetes\_web\_app.py} & 2070-2095 & Flask route receives POST request \\
3 & \texttt{autoscaling\_integration.py} & 124-148 & Integration layer coordinates \\
4 & \texttt{predictive\_autoscaler.py} & 631-778 & Gets recommendation (calls LLM) \\
5 & \texttt{llm\_autoscaling\_advisor.py} & 102-270 & Main LLM processing logic \\
5a & \texttt{llm\_autoscaling\_advisor.py} & 325-376 & Prepares context dictionary \\
5b & \texttt{llm\_autoscaling\_advisor.py} & 378-406 & Creates system prompt \\
5c & \texttt{llm\_autoscaling\_advisor.py} & 408-457 & Creates user prompt \\
5d & \texttt{llm\_autoscaling\_advisor.py} & 191-199 & Calls Groq API \\
5e & \texttt{llm\_autoscaling\_advisor.py} & 272-300 & Parses JSON response \\
6 & \texttt{templates/autoscaling.html} & 735-888 & Displays recommendation in UI \\
\bottomrule
\end{tabular}
\caption{End-to-End Flow File Breakdown}
\end{table}

\section{Example API Request/Response}

\subsection{Request}

\textbf{File:} \texttt{llm\_autoscaling\_advisor.py}\\
\textbf{Method:} \texttt{analyze\_scaling\_decision()} (lines 191-199)

\textbf{Actual API Request:}
\begin{lstlisting}[style=python, caption={Groq API Request}]
response = self.client.chat.completions.create(
    model="llama-3.1-8b-instant",  # Primary model
    messages=[
        {
            "role": "system",
            "content": "You are an expert Kubernetes autoscaling advisor with deep knowledge of:\n- Resource optimization and cost management\n..."
        },
        {
            "role": "user",
            "content": "Analyze the following Kubernetes deployment autoscaling scenario and provide a recommendation:\n\n**Deployment Information:**\n- Name: test-app-autoscaling\n..."
        }
    ],
    temperature=0.3,
    max_tokens=1000
)
\end{lstlisting}

\textbf{HTTP Request Details:}
\begin{itemize}
    \item \textbf{Endpoint:} \texttt{POST https://api.groq.com/openai/v1/chat/completions}
    \item \textbf{Headers:}
    \begin{itemize}
        \item \texttt{Authorization: Bearer gsk\_...} (API key)
        \item \texttt{Content-Type: application/json}
    \end{itemize}
    \item \textbf{Request Body:} JSON as shown above
\end{itemize}

\subsection{Response}

\textbf{File:} \texttt{llm\_autoscaling\_advisor.py}\\
\textbf{Method:} \texttt{\_parse\_llm\_response()} (lines 272-300)

\textbf{Raw LLM Response (from Groq API):}
\begin{lstlisting}[style=json, caption={Raw Groq API Response}]
{
    "id": "chatcmpl-abc123",
    "object": "chat.completion",
    "created": 1701234567,
    "model": "llama-3.1-8b-instant",
    "choices": [{
        "index": 0,
        "message": {
            "role": "assistant",
            "content": "{\n  \"action\": \"scale_down\",\n  \"target_replicas\": 8,\n  \"confidence\": 0.8,\n  \"reasoning\": \"Based on the current resource usage (CPU: 169.2%, Memory: 11.7%) and predicted future demand...\",\n  \"factors_considered\": [\"Current resource pressure\", \"Predicted future demand\", \"Cost optimization opportunities\"],\n  \"risk_assessment\": \"low\",\n  \"cost_impact\": \"low\",\n  \"performance_impact\": \"neutral\",\n  \"recommended_timing\": \"immediate\"\n}"
        },
        "finish_reason": "stop"
    }],
    "usage": {
        "prompt_tokens": 450,
        "completion_tokens": 180,
        "total_tokens": 630
    }
}
\end{lstlisting}

\textbf{Parsed Recommendation (extracted from response):}
\begin{lstlisting}[style=json, caption={Parsed Recommendation}]
{
    "action": "scale_down",
    "target_replicas": 8,
    "confidence": 0.8,
    "reasoning": "Based on the current resource usage (CPU: 169.2%, Memory: 11.7%) and predicted future demand (CPU/Memory predictions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), it is likely that the current replica count of 10 is more than sufficient. The forecast indicates no significant increase in demand over the next 6 hours, and scaling down to 8 replicas will reduce costs while maintaining performance.",
    "factors_considered": [
        "Current resource pressure",
        "Predicted future demand",
        "Cost optimization opportunities",
        "Performance requirements",
        "Stability factors"
    ],
    "risk_assessment": "low",
    "cost_impact": "low",
    "performance_impact": "neutral",
    "recommended_timing": "immediate"
}
\end{lstlisting}

\textbf{Final Return Value} (from \texttt{analyze\_scaling\_decision()}):
\begin{lstlisting}[style=python, caption={Final Return Value}]
{
    'success': True,
    'recommendation': {
        'action': 'scale_down',
        'target_replicas': 8,
        'confidence': 0.8,
        'reasoning': '...',
        # ... other fields
    },
    'llm_model': 'llama-3.1-8b-instant',
    'timestamp': '2025-12-02T13:22:55.123456',
    'cached': False
}
\end{lstlisting}

\vspace{1cm}

\noindent\textbf{Document Version:} 1.0\\
\textbf{Last Updated:} December 2025\\
\textbf{Status:} Final

\end{document}

