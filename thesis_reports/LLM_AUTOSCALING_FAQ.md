# LLM Autoscaling - Frequently Asked Questions

**Author:** Pedram Nikjooy  
**Thesis:** AI Agent for Kubernetes Management  
**Date:** December 2025

---

## Q1: How are confidence scores calculated? Are they static?

### Answer: **Confidence scores are NOT calculated by the system - they are generated by the LLM itself.**

**How it works:**

1. **LLM Generates Confidence:** The Groq LLM is instructed in the system prompt to provide a confidence score (0.0-1.0) in its JSON response. The LLM evaluates its own confidence based on:
   - Quality of input data
   - Clarity of the scaling scenario
   - Certainty of the recommendation
   - Consistency of metrics and forecasts

2. **System Extracts Confidence:** The system simply extracts the confidence value from the LLM's JSON response.

**Code Location:**
- **File:** `llm_autoscaling_advisor.py`
- **Method:** `_parse_llm_response()` (lines 272-300)
- **Extraction:** `confidence = llm_output.get('confidence', 0.5)`

**Fallback Behavior:**
- If LLM doesn't provide confidence: Defaults to `0.5` (50%)
- If JSON parsing fails: Uses `0.5` in fallback recommendation (line 316)

**Example:**
```python
# LLM Response JSON:
{
    "action": "scale_down",
    "target_replicas": 8,
    "confidence": 0.8,  # ← LLM generates this
    "reasoning": "..."
}

# System extracts it:
confidence = recommendation.get('confidence', 0.5)  # Gets 0.8
```

**Conclusion:** Confidence scores are **dynamic** and **LLM-generated**, not static or calculated by the system.

---

### Q1.1: How are confidence scores generated by the LLM itself?

**Answer: The LLM generates confidence scores through its internal reasoning process, evaluating multiple factors to assess how certain it is about its recommendation.**

**Mechanism:**

The LLM (Llama 3.1) uses its training and reasoning capabilities to self-assess confidence. Here's how:

#### 1. **System Prompt Instructions**

The system prompt instructs the LLM to provide a confidence score, but doesn't specify how to calculate it - the LLM determines this internally:

**File:** `llm_autoscaling_advisor.py`, method `_create_system_prompt()` (lines 378-406)

```python
Respond in JSON format with:
{
  "action": "scale_up" | "scale_down" | "maintain" | "at_max",
  "target_replicas": <number>,
  "confidence": <0.0-1.0>,  # ← LLM must provide this
  ...
}
```

#### 2. **LLM's Internal Confidence Assessment**

The LLM evaluates confidence based on:

**a) Data Quality and Completeness:**
- Are all required metrics present? (CPU, Memory, Forecasts, HPA status)
- Are metrics consistent and reasonable?
- Is there sufficient historical data for forecasts?

**b) Scenario Clarity:**
- Is the scaling need obvious? (e.g., CPU at 180% = clear scale-up need)
- Are metrics ambiguous? (e.g., CPU high but memory low = less certain)
- Are forecasts conflicting? (e.g., CPU increasing but memory decreasing)

**c) Recommendation Certainty:**
- **High Confidence (0.8-1.0):** Clear, unambiguous scenarios
  - Example: CPU at 180%, forecast shows increasing trend, HPA stable → High confidence to scale up
  - Example: CPU at 10%, memory at 5%, forecast flat → High confidence to scale down
  
- **Medium Confidence (0.5-0.7):** Some uncertainty
  - Example: CPU at 75% (near threshold), forecast unclear → Medium confidence
  - Example: Conflicting signals (high CPU but low memory) → Medium confidence
  
- **Low Confidence (0.0-0.4):** High uncertainty
  - Example: Missing forecast data, inconsistent metrics → Low confidence
  - Example: At capacity limits, unclear optimal action → Low confidence

**d) Pattern Recognition:**
- Does the scenario match known patterns from training?
- Are there edge cases or unusual situations?
- Is the recommendation aligned with best practices?

#### 3. **Example LLM Reasoning Process**

When the LLM receives this context:
```
CPU: 169.2% (high)
Memory: 11.7% (low)
Forecast: CPU/Memory predictions [0.0, 0.0, 0.0, 0.0, 0.0, 0.0] (flat, no increase)
HPA: Stable, 10 replicas
```

**LLM's Internal Assessment:**
1. **Data Quality:** ✅ All metrics present, consistent
2. **Scenario:** ⚠️ Mixed signals - high CPU but low memory, forecast shows no increase
3. **Certainty:** Medium - CPU suggests scale-up, but forecast suggests no need
4. **Confidence:** 0.7-0.8 (medium-high) - Clear that scale-down is safe, but CPU is high

**LLM Output:**
```json
{
    "action": "scale_down",
    "target_replicas": 8,
    "confidence": 0.8,  # ← LLM determined this based on its assessment
    "reasoning": "Based on the current resource usage (CPU: 169.2%, Memory: 11.7%) 
                  and predicted future demand (CPU/Memory predictions: [0.0, ...]), 
                  there is an opportunity to scale down..."
}
```

#### 4. **Factors That Influence LLM Confidence**

**High Confidence Scenarios:**
- Clear, unambiguous metrics (e.g., CPU consistently > 80%)
- Consistent forecasts (all predictions show same trend)
- Aligned signals (CPU and Memory both high/low)
- Stable HPA status
- Well-defined constraints (clear min/max replicas)

**Low Confidence Scenarios:**
- Missing or incomplete data
- Conflicting signals (CPU high, Memory low)
- Unclear forecasts (mixed trends)
- Edge cases (at max replicas, unusual patterns)
- Ambiguous scenarios (metrics near thresholds)

#### 5. **LLM Model Behavior**

**Model:** `llama-3.1-8b-instant`
- **Temperature:** 0.3 (low) - This makes confidence scores more consistent
- **Training:** The model was trained on diverse data, learning patterns of certainty/uncertainty
- **Self-Assessment:** The model has learned to assess its own certainty through training

**Why This Works:**
- LLMs are trained to recognize patterns and assess certainty
- The model has seen many scenarios during training and learned when to be confident
- Lower temperature (0.3) ensures more consistent confidence assessments
- The model evaluates the "strength of evidence" for its recommendation

#### 6. **Code Implementation**

**File:** `llm_autoscaling_advisor.py`

```python
# System prompt instructs LLM to provide confidence
system_prompt = """...
  "confidence": <0.0-1.0>,  # LLM must assess and provide this
..."""

# LLM generates response with confidence
response = self.client.chat.completions.create(
    model="llama-3.1-8b-instant",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt_with_context}
    ],
    temperature=0.3  # Low temperature = more consistent confidence scores
)

# LLM's internal reasoning produces confidence score
llm_output = response.choices[0].message.content
# Contains: {"confidence": 0.8, ...}  ← LLM determined this internally
```

**Conclusion:** The LLM generates confidence scores through its internal reasoning process, evaluating data quality, scenario clarity, recommendation certainty, and pattern recognition. It's not a formula - it's the LLM's self-assessment of how certain it is about its recommendation based on the evidence provided.

---

## Q2: Does predictive autoscaling depend on HPA autoscaling?

### Answer: **No, predictive autoscaling is NOT dependent on HPA. It can work independently OR alongside HPA.**

**How it works:**

1. **Standalone Operation:** Predictive autoscaling can scale deployments directly using `kubectl scale` without requiring HPA.

2. **HPA Integration (Optional):** If HPA doesn't exist, predictive autoscaling can create one. If HPA exists, it works alongside it.

**Code Evidence:**

**File:** `predictive_autoscaler.py`, method `predict_and_scale()` (lines 164-244)

```python
# Check if HPA exists
hpa_name = f"{deployment_name}-hpa"
hpa_exists = self.hpa_manager.get_hpa(hpa_name, namespace)['success']

if not hpa_exists:
    # Create HPA with predictive settings (optional)
    hpa_result = self.hpa_manager.create_hpa(...)
    # Also scale deployment directly
    self._scale_deployment(deployment_name, namespace, required_replicas)
else:
    # HPA exists - scale deployment directly (works alongside HPA)
    self._scale_deployment(deployment_name, namespace, required_replicas)
```

**Direct Scaling Method:**
- **File:** `predictive_autoscaler.py`, method `_scale_deployment()` (lines 608-629)
- **Implementation:** Uses `kubectl scale deployment` command directly
- **No HPA Required:** This works independently of HPA

```python
def _scale_deployment(self, deployment_name: str, namespace: str, replicas: int):
    result = subprocess.run(
        ['kubectl', 'scale', 'deployment', deployment_name,
         f'--replicas={replicas}', '-n', namespace],
        ...
    )
```

**Conclusion:** Predictive autoscaling is **independent** of HPA. It can:
- Work standalone (scales directly via kubectl)
- Create HPA if needed (for reactive scaling backup)
- Work alongside existing HPA (both systems operate together)

---

## Q3: Does predictive autoscaling actually implement autoscaling, or is it just recommendations?

### Answer: **It does BOTH - there are two different methods:**

### Method 1: `get_scaling_recommendation()` - **ONLY RECOMMENDATIONS**
- **Purpose:** Returns recommendations without implementing scaling
- **Used by:** Web UI to display recommendations
- **File:** `predictive_autoscaler.py` (lines 631-778)
- **Does NOT scale:** Only returns recommendation object

### Method 2: `predict_and_scale()` - **ACTUALLY IMPLEMENTS SCALING**
- **Purpose:** Gets recommendation AND executes scaling
- **Used by:** When user enables predictive autoscaling
- **File:** `predictive_autoscaler.py` (lines 62-319)
- **DOES scale:** Calls `_scale_deployment()` which runs `kubectl scale`

**Code Flow:**

**When User Enables Predictive Autoscaling:**
```python
# File: autoscaling_integration.py, method enable_predictive_autoscaling() (lines 124-148)

# Step 1: Get recommendation (for display)
recommendation = self.predictive_autoscaler.get_scaling_recommendation(...)

# Step 2: Actually implement scaling
result = self.predictive_autoscaler.predict_and_scale(...)  # ← This scales!
```

**Inside `predict_and_scale()`:**
```python
# File: predictive_autoscaler.py, lines 193, 223, 248

# Actually scales the deployment:
self._scale_deployment(deployment_name, namespace, required_replicas)
```

**Inside `_scale_deployment()`:**
```python
# File: predictive_autoscaler.py, lines 615-617

# Executes kubectl command:
subprocess.run([
    'kubectl', 'scale', 'deployment', deployment_name,
    f'--replicas={replicas}', '-n', namespace
])
```

**Conclusion:** 
- **Recommendations:** `get_scaling_recommendation()` - only returns recommendations
- **Implementation:** `predict_and_scale()` - actually scales deployments using `kubectl scale`
- **When enabled:** Both are called - recommendation for display, then actual scaling

---

## Q4: Does the LLM AI create JSON that implements autoscaling?

### Answer: **Yes, but indirectly. The LLM creates JSON recommendations that are then used to implement autoscaling.**

**Complete Flow:**

### Step 1: LLM Generates JSON Recommendation
- **File:** `llm_autoscaling_advisor.py`, method `analyze_scaling_decision()` (lines 191-222)
- **LLM Output:** JSON with `action` and `target_replicas`
- **Example JSON:**
```json
{
    "action": "scale_down",
    "target_replicas": 8,
    "confidence": 0.8,
    "reasoning": "..."
}
```

### Step 2: JSON is Used by Predictive Autoscaler
- **File:** `predictive_autoscaler.py`, method `get_scaling_recommendation()` (lines 716-728)
- **Extracts:** `target_replicas` from LLM JSON
- **Creates Action:** Uses LLM's `action` and `target_replicas`

```python
# Get LLM recommendation
llm_result = self.llm_advisor.get_intelligent_recommendation(...)

if llm_result.get('success'):
    llm_recommendation = llm_result.get('recommendation', {})
    # llm_recommendation contains: action, target_replicas, etc.
```

### Step 3: Action is Used to Implement Scaling
- **File:** `predictive_autoscaler.py`, method `predict_and_scale()` (lines 156-248)
- **Uses:** `action['target_replicas']` from LLM recommendation
- **Implements:** Calls `_scale_deployment()` with LLM's target replicas

```python
# Determine scaling action (uses LLM recommendation if available)
action = self._determine_scaling_action(...)  # May use LLM recommendation

if action['action'] == 'scale_up':
    required_replicas = action['target_replicas']  # From LLM JSON
    self._scale_deployment(deployment_name, namespace, required_replicas)  # ← Implements!
```

### Step 4: Actual Kubernetes Scaling
- **File:** `predictive_autoscaler.py`, method `_scale_deployment()` (lines 608-629)
- **Executes:** `kubectl scale deployment <name> --replicas=<target_replicas>`
- **Result:** Deployment is actually scaled in Kubernetes

```python
def _scale_deployment(self, deployment_name: str, namespace: str, replicas: int):
    # replicas comes from LLM's target_replicas
    subprocess.run([
        'kubectl', 'scale', 'deployment', deployment_name,
        f'--replicas={replicas}', '-n', namespace  # ← LLM's target_replicas used here
    ])
```

**Complete Chain:**
```
LLM JSON → Extract target_replicas → predict_and_scale() → _scale_deployment() → kubectl scale
```

**Conclusion:** 
- **LLM creates JSON** with `action` and `target_replicas`
- **System extracts** `target_replicas` from LLM JSON
- **System implements** scaling using LLM's `target_replicas` via `kubectl scale`
- **Result:** LLM's recommendation is actually executed in Kubernetes

---

## Q5: Why do I see recommendations but "Predictive Enabled: 0" and replicas haven't changed?

### Answer: **Recommendations are displayed automatically, but predictive autoscaling must be ENABLED to actually implement scaling.**

**The Issue:**

You're seeing:
- ✅ **LLM Recommendation:** "Target Replicas: 5" (displayed)
- ❌ **Predictive Enabled:** 0 (not enabled)
- ❌ **Total Replicas:** 10 (unchanged)

**Why This Happens:**

### Two Different Functions:

1. **`get_scaling_recommendation()`** - **ALWAYS AVAILABLE**
   - **Purpose:** Shows recommendations for ANY deployment
   - **Called by:** Web UI automatically when viewing autoscaling page
   - **File:** `predictive_autoscaler.py` (lines 631-778)
   - **Does NOT scale:** Only returns recommendation object
   - **No enabling required:** Works for any deployment

2. **`predict_and_scale()`** - **REQUIRES ENABLING**
   - **Purpose:** Actually implements scaling
   - **Called by:** Only when you click "Enable Predictive Autoscaling" button
   - **File:** `predictive_autoscaler.py` (lines 62-319)
   - **DOES scale:** Executes `kubectl scale` command
   - **Requires enabling:** Must be explicitly enabled per deployment

**Code Flow:**

**When You View Autoscaling Page:**
```python
# File: templates/autoscaling.html, function loadPredictiveForecast() (lines 620-732)

# Automatically calls:
fetch(`/api/autoscaling/recommendations/${serverId}?deployment=${deployment}&namespace=${namespace}`)
    ↓
# Backend: ai_kubernetes_web_app.py, route get_scaling_recommendations() (lines 2133-2157)
    ↓
# Calls: autoscaling.get_scaling_recommendations()
    ↓
# Calls: predictive_autoscaler.get_scaling_recommendation()  ← ONLY RECOMMENDATIONS
    ↓
# Returns recommendation JSON (does NOT scale)
```

**When You Click "Enable Predictive Autoscaling":**
```python
# File: templates/autoscaling.html, function enablePredictiveAutoscaling() (lines 573-596)

# Calls:
fetch(`/api/autoscaling/predictive/enable/${serverId}`, {method: 'POST', ...})
    ↓
# Backend: ai_kubernetes_web_app.py, route enable_predictive_autoscaling() (lines 2107-2132)
    ↓
# Calls: autoscaling.enable_predictive_autoscaling()
    ↓
# Calls: predictive_autoscaler.predict_and_scale()  ← ACTUALLY SCALES!
    ↓
# Executes: kubectl scale deployment ... --replicas=5
```

**How to Actually Implement Scaling:**

1. **Fill out the form:**
   - Deployment Name: `test-app-autoscaling` (or your deployment)
   - Namespace: `ai4k8s-test` (or your namespace)
   - Min Replicas: `2`
   - Max Replicas: `10`

2. **Click "Enable Predictive Autoscaling" button**

3. **What happens:**
   - System calls `predict_and_scale()` (not just `get_scaling_recommendation()`)
   - System executes `kubectl scale deployment test-app-autoscaling --replicas=5`
   - Deployment is actually scaled to 5 replicas
   - "Predictive Enabled" count increases to 1
   - Total Replicas updates to reflect new count

**Code Evidence:**

**Recommendations (No Scaling):**
```python
# File: predictive_autoscaler.py, method get_scaling_recommendation() (line 631)
def get_scaling_recommendation(self, ...):
    """Get scaling recommendation without executing"""
    # ... gets LLM recommendation ...
    return {
        'success': True,
        'recommendation': action,  # ← Only returns recommendation
        # NO SCALING HAPPENS HERE
    }
```

**Implementation (Actual Scaling):**
```python
# File: predictive_autoscaler.py, method predict_and_scale() (line 62)
def predict_and_scale(self, ...):
    """Predict future load and scale proactively"""
    # ... gets recommendation ...
    if action['action'] == 'scale_down':
        required_replicas = action['target_replicas']  # From LLM: 5
        self._scale_deployment(deployment_name, namespace, required_replicas)  # ← SCALES!
        # kubectl scale deployment ... --replicas=5
```

**Why "Predictive Enabled: 0"?**

The count comes from deployments with the annotation `ai4k8s.io/predictive-autoscaling=enabled`:

**File:** `predictive_autoscaler.py`, method `list_enabled_deployments()` (lines 502-550)

```python
# Checks Kubernetes annotations:
annotations = deployment['metadata'].get('annotations', {})
if annotations.get('ai4k8s.io/predictive-autoscaling-enabled') == 'true':
    # This deployment has predictive autoscaling enabled
```

**When you click "Enable Predictive Autoscaling":**
- System adds annotation: `ai4k8s.io/predictive-autoscaling-enabled: "true"`
- System scales deployment using LLM's recommendation
- "Predictive Enabled" count increases

**Summary:**

| What You See | What It Means | Action Required |
|--------------|---------------|-----------------|
| **LLM Recommendation: "Target Replicas: 5"** | Recommendation displayed | None - just viewing |
| **Predictive Enabled: 0** | No deployments have predictive autoscaling enabled | Click "Enable Predictive Autoscaling" |
| **Total Replicas: 10** | Current replica count (unchanged) | Will change after enabling |

**To Actually Scale:**
1. Make sure you're viewing the correct deployment
2. Fill out the "Enable Predictive Autoscaling" form
3. Click the "Enable Predictive Autoscaling" button
4. The system will:
   - Enable predictive autoscaling for that deployment
   - Execute the scaling (change replicas from 10 to 5)
   - Update "Predictive Enabled" count to 1
   - Update "Total Replicas" to 5

**Conclusion:** Recommendations are **always visible** (for informational purposes), but scaling is **only implemented** when you explicitly enable predictive autoscaling by clicking the button.

---

## Q6: Why does predictive autoscaling show LLM recommendation to scale down, but doesn't actually scale down?

### Answer: **This was a bug - `predict_and_scale()` was using rule-based logic instead of LLM recommendations. Now fixed!**

**The Problem:**

When you enabled predictive autoscaling, the system was:
1. ✅ Showing LLM recommendation: "Scale Down to 5 replicas" (from `get_scaling_recommendation()`)
2. ❌ But actually scaling using rule-based logic (from `predict_and_scale()`)
3. ❌ Rule-based logic saw CPU at 191% (> 75% threshold) and said "no action" or "scale up"

**Root Cause:**

**File:** `predictive_autoscaler.py`, method `predict_and_scale()` (line 156)

**Before Fix:**
```python
# Only used rule-based logic
action = self._determine_scaling_action(
    max_predicted_cpu,
    max_predicted_memory,
    current_replicas,
    min_replicas,
    max_replicas
)
```

**Rule-Based Logic Issues:**
- **Scale Down Threshold:** Requires BOTH CPU < 25% AND Memory < 25%
- **Your Metrics:** CPU 191%, Memory 11.7%
- **Result:** CPU 191% > 25%, so rule-based logic says "no scale down"
- **LLM Logic:** Considers forecasts showing no increase → recommends scale down

**The Fix:**

Updated `predict_and_scale()` to use LLM recommendations when available, just like `get_scaling_recommendation()` does:

**After Fix:**
```python
# Try LLM recommendation first
if self.use_llm and self.llm_advisor:
    llm_result = self.llm_advisor.get_intelligent_recommendation(...)
    if llm_result.get('success'):
        llm_recommendation = llm_result.get('recommendation', {})
        action = {
            'action': llm_recommendation.get('action'),
            'target_replicas': llm_recommendation.get('target_replicas'),
            ...
        }
        # Use LLM recommendation for scaling
    else:
        # Fallback to rule-based
        action = self._determine_scaling_action(...)
else:
    # No LLM, use rule-based
    action = self._determine_scaling_action(...)
```

**Now:**
- ✅ `predict_and_scale()` uses LLM recommendations when available
- ✅ LLM's "scale down to 5" recommendation is actually implemented
- ✅ Falls back to rule-based logic only if LLM unavailable or rate-limited

**To Test:**

1. Enable predictive autoscaling again (or it should work automatically if already enabled)
2. The system will now use LLM recommendation to scale down to 5 replicas
3. Check: `kubectl get deployment test-app-autoscaling -n ai4k8s-test` should show 5 replicas

**Conclusion:** The bug has been fixed. `predict_and_scale()` now uses LLM recommendations for actual scaling, not just rule-based logic.

---

## Summary Table

| Question | Answer | Key Point |
|----------|--------|-----------|
| **Confidence scores** | LLM-generated, not calculated | Dynamic, comes from LLM's self-assessment |
| **HPA dependency** | Independent, optional integration | Can work standalone or with HPA |
| **Recommendations vs Implementation** | Both exist | `get_scaling_recommendation()` = recommendations only<br>`predict_and_scale()` = actually scales |
| **LLM JSON → Autoscaling** | Yes, indirectly | LLM JSON → extract target_replicas → kubectl scale |
| **Why recommendations but no scaling?** | Recommendations auto-display, scaling requires enabling | Must click "Enable Predictive Autoscaling" to actually scale |

---

**Document Version:** 1.1  
**Last Updated:** December 2025
