\documentclass[sigconf]{acmart}
\usepackage{tikz}
\usetikzlibrary{positioning, arrows.meta, shapes.geometric, fit, calc}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{array}
\usepackage{algorithmic}
\usepackage{xcolor}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}


\setcopyright{acmlicensed}
\copyrightyear{2026}
\acmYear{2026}
\acmDOI{XXXXXXX.XXXXXXX}

\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}

\acmISBN{978-1-4503-XXXX-X/2018/06}


\newcommand{\Solution}{\texttt{AutoSage}}
%% AutoSage K8s
% PodMind AI
\newcommand{\as}[1]{{\color{red}{#1}}}
\newcommand{\cz}[1]{{\color{brown}{#1}}}
\newcommand{\pn}[1]{{\color{blue}{#1}}} 

\begin{document}

\title{Agentic AI for Reliable Kubernetes Operations: Combining Large Language Models with Decision Analysis
%Agentic AI for Automatic Kubernetes Operations and Autoscaling
}

\author{Anonymous Authors}

\iffalse
\author{Pedram Nikjooy}
\affiliation{
  \institution{Politecnico di Torino}
  \city{Torino}
  \country{Italy}}
\email{pedram.nikjooy@studenti.polito.it}

\author{Cristian Zilli}
\affiliation{
 \institution{Politecnico di Torino}
 \city{Torino}
 \country{Italy}}
\email{cristian.zilli@polito.it}

\author{Alessio Sacco}
\affiliation{%
  \institution{Politecnico di Torino}
  \city{Torino}
  \country{Italy}}
\email{alessio_sacco@polito.it}

\renewcommand{\shortauthors}{Nikjooy et al.}
\fi

\begin{abstract}
The widespread adoption of cloud computing technologies enabled notable consolidation of computational resources and unprecedented service agility, with Kubernetes (K8s) as the default open-source container orchestration platform that automates application management. However, managing K8s clusters at scale demands continuous monitoring, accurate resource forecasting, and timely autoscaling decisions---tasks that exceed the capacity of static, threshold-based controllers. To efficiently orchestrate K8s operations, we present \Solution, an agentic AI platform that combines Large Language Model (LLM) reasoning with a more formal multi-criteria optimization. The system ingests real-time cluster metrics %through the Kubernetes Metrics API 
and feeds them into a predictive monitoring pipeline comprising linear-trend forecasting with %bootstrap-based 
uncertainty quantification and %Isolation-Forest 
anomaly detection. %calibrated via Platt scaling.
A context-aware LLM advisor analyzes forecasts alongside deployment state and generates scaling recommendations (i.e., suggestions to scale up or down) for both Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA) resources. Each LLM recommendation is cross-validated by a %TOPSIS-based 
Multi-Criteria Decision Analysis (MCDA) module that evaluates candidate scaling actions against performance and cost criteria %five criteria---cost, performance, stability, forecast alignment, and response time---and can override the LLM when the MCDA dominance margin exceeds a configurable threshold.
and drives LLM towards a more stable configuration.
%All components are exposed through a web dashboard that visualizes prediction intervals, exceedance probabilities, uncertainty decomposition, and calibrated anomaly severities, enabling operators to assess both the scaling decision and its confidence.
As we envision the cloud operator as responsible for such decisions, we present these recommendations (along with predictions and uncertainty estimates) via a web API for the final decision.
We deployed this solution over a local testbed and demonstrated end-to-end agentic autoscaling with transparent, auditable decision-making.
\end{abstract}

\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010521.10010537.10010540</concept_id>
  <concept_desc>Computer systems organization~Cloud computing</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010178.10010187</concept_id>
  <concept_desc>Computing methodologies~Reasoning about belief and knowledge</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010257.10010293.10010294</concept_id>
  <concept_desc>Computing methodologies~Neural networks</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10002944.10011123.10010916</concept_id>
  <concept_desc>General and reference~Performance</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Cloud computing}
\ccsdesc[300]{Computing methodologies~Reasoning about belief and knowledge}
\ccsdesc[300]{Computing methodologies~Neural networks}
\ccsdesc[100]{General and reference~Performance}

\keywords{Kubernetes autoscaling, large language models, MCDA, agentic AI, cloud operations}

\maketitle

\section{Introduction}
\label{sec:introduction}

Traditional cloud computing operations often require complex manual configurations, especially for deploying services in containerized environments (\textit{e.g.,} microservices). Kubernetes (K8s) has become the de facto open-source platform for container orchestration, enabling automated software deployment, scaling, and management. However, defining deployment specifications, such as resource requests and limits, still demands substantial expertise and remains challenging even for experienced practitioners~\cite{yu2020microscaler, al2024containerized, sacco2025intent}.
%In particular, despite the automation provided by the K8s suite, operations like capacity planning, resource optimization, and scaling remain challenging. This is due to the complexity of tasks that are required to achieve a system that meets provisioning targets reliably. Examples of these tasks are predicting resource demands, optimal QoS-compliant workload packing, stable scaling that accounts for dependencies among services and application statefulness, gathering telemetry, and safeguarding automated operations.
In particular, despite K8s's built-in automation, reliably meeting provisioning targets remains complex, requiring accurate demand forecasting, QoS-aware workload packing, dependency- and state-aware scaling, robust telemetry collection, and safeguards for automated operations.

%Overcoming these hurdles can prove both labor-intensive and difficult with traditional tools as infrastructures scale massively. For this reason, integrating Agentic Artificial Intelligence (Agentic AI) into orchestration workflows has become the focus of many research efforts and has led to the emergence of AIOps~\cite{shetty2024building}, a design paradigm that shifts from predominantly human-driven processes toward the autonomous AI-enabled management of complex IT and cloud infrastructures. The incorporation of AI-driven techniques into large-scale cloud infrastructure management, which is comprised of a multitude of complex workflows, introduces both substantial opportunities and non-trivial challenges. Embedding AI models within these workflows can assist with high-dimensional telemetry analysis, workload prediction, and adaptive resource optimization, capabilities that are increasingly difficult to achieve using traditional mechanisms alone. In the specific case of capacity management, traditional threshold-based and reactive scaling mechanisms are often insufficient to handle dynamic traffic patterns and the delayed effects of scaling decisions, both inherent to distributed systems. These limitations motivate the exploration of AI-driven resource management approaches for k8s capacity planning, leveraging the strengths of reasoning-capable models to enable proactive management by factoring in application behavior, container-level resource specifications, and cluster-level scheduling dynamics when devising capacity plans.
Addressing these challenges with traditional tools becomes increasingly labor-intensive, motivating the integration of Agentic AI into orchestration workflows under the AIOps paradigm~\cite{shetty2024building}. This shift toward autonomous and AI-enabled management enables advanced telemetry analysis, workload prediction, and adaptive resource optimization across complex cloud environments. In capacity management specifically, traditional mechanisms, \textit{i.e.,} reactive and threshold-based scaling~\cite{yu2020microscaler, rossi2020self, rossi2020geo, xu2025auto, ahmad2025towards}, often fail to cope with dynamic traffic and delayed scaling effects in distributed systems. This prompts the adoption of AI-driven approaches for K8s planning that incorporate application behavior, container-level specifications, and cluster-level scheduling dynamics to enable proactive, context-aware resource management~\cite{khaleq2021intelligent, nguyen2022graph, song2023chainsformer, subramaniam2023automated, santos2023gym, quattrocchi2024autoscaling}.

%Nonetheless, the practical adoption of AIOps and agentic AI within K8s-based infrastructures raises several technical challenges. First, identifying appropriate operational use cases remains non-trivial, as not all tasks benefit equally from learning-based automation. Second, effective model performance depends on the availability and quality of observability data, such as metrics, logs, traces, and events collected across clusters. Data sparsity, noise, and cross-layer dependencies can significantly limit model reliability. Third, AI systems must be tightly integrated with control-plane APIs and policy frameworks to safely and timely actuate changes in live environments. This requirement introduces concerns related to model deployment in resource-constrained clusters, scalability across multi-cluster or multi-cloud infrastructures, and the interpretability of AI-generated decisions—particularly when those decisions affect service availability or compliance constraints.
%Furthermore, granting high levels of autonomy to AI agents further amplifies safety and governance considerations. In production cloud systems, erroneous scaling actions, configuration updates, or remediation procedures may result in cascading failures or service disruption for end users, potentially breaking Service Level Agreements (SLA). Consequently, AI-driven control loops must incorporate safeguard mechanisms such as policy validation, rollback strategies, bounded action spaces, and, where appropriate, human-in-the-loop supervision. Ensuring transparency, auditability, and explainability of automated decisions remains a critical requirement for operational trust and regulatory alignment.
Nonetheless, the practical adoption of AIOps and agentic AI within K8s-based infrastructures raises several key technical challenges: %\textit{(i)} \as{questo è da chiarire meglio, ho provato a rifrasare ma non capivo bene, non ero così convinto di cosa volessi dire} use-case selection, as not all operational tasks equally benefit from learning-based automation; 
\pn{Applying AIOps and agentic AI in Kubernetes is promising, but still difficult in practice.
First, not every operational task is suitable for AI automation, so selecting the right use cases is essential.
Second, model quality depends on observability data quality; noisy, sparse, or inconsistent metrics/logs/traces can reduce reliability.
Third, safe operation requires tight integration with Kubernetes control-plane APIs and policies, especially for live actuation.
This creates additional constraints in resource-limited clusters and multi-cluster environments, and increases the need for explainable, auditable decisions when availability or compliance is affected.}
\textit{(i)} data quality and observability, since reliable performance depends on high-quality metrics, logs, traces, and events, often hindered by noise, sparsity, and cross-layer dependencies; \textit{(ii)} system integration, requiring tight coupling with control-plane APIs and policy frameworks to safely actuate changes in live, potentially resource-constrained and multi-cluster environments; and \textit{(iii)} interpretability, safety operations, and governance, particularly when AI-driven decisions impact availability or compliance. Increasing agent autonomy amplifies safety concerns, where erroneous scaling actions, configuration updates, or remediation procedures may result in cascading failures, service disruption, or SLA violations for end users. This challenge introduces the need for safeguards such as policy validation, rollback mechanisms, bounded action spaces, human-in-the-loop supervision (where deemed), and strong transparency, auditability, and explainability.


To achieve these objectives, we introduce \Solution{}, our agentic AI platform engineered to address the challenges of autonomous capacity management. Our solution supports K8s operations, leveraging LLMs to make proactive, dynamic, and context-aware auto-scaling decisions while integrating safeguards and human-in-the-loop mechanisms. We demonstrate that our solution...
%some results

The remainder of this paper is structured as follows: Section \ref{sec:related} provides an overview of related work and outlines our contribution. Section \ref{sec:system} comprehensively illustrates the architecture of \Solution{} and its components. Section \ref{sec:evaluation} presents a comparative evaluation of our solution. Section \ref{sec:conclusion} concludes the paper and summarizes our results.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/overview.pdf}
    \caption{Overview of \Solution{}'s interaction with K8s ecosystem.}
    \label{fig:overview}
\end{figure*}

\section{Related Work}
\label{sec:related}

Recent advances in AIOps increasingly position LLMs as the central reasoning component in cloud infrastructure management, complementing or replacing traditional machine learning (ML) (e.g.~\cite{liu2022scanflow}) and human-centered pipelines. Beyond predictive analytics, this shift reflects the emergence of agentic AI systems capable of autonomous reasoning, tool use, and iterative decision-making across operational workflows.

Existing literature primarily focuses on failure and incident management in cloud platforms~\cite{zhang2025survey}. In this context, AI agents have been explored for anomaly detection, root cause analysis (RCA), and automated failure mitigation, with growing industrial adoption. Unlike static ML models, agentic systems can dynamically interpret logs, query external knowledge sources, and orchestrate remediation actions, enabling more adaptive and context-aware operations.
%
For instance,~\cite{delacruzcabello2025aiops} introduces an LLM-based framework for log anomaly detection, showing that with augmentation techniques such as fine-tuning, RAG, prompt engineering, or ensembles, LLMs can surpass traditional methods. This work underscores the shift toward LLM-driven agents that integrate reasoning with operational context.
%
Following this modus operandi, LogFiT~\cite{almodovar2024logfit} and FlexLog~\cite{hadadi2025llm} are able to deal with the highly dynamical nature of log data, respectively via model fine-tuning (LogFiT) and via an ensemble-based approach with other ML models (FlexLog).
%with the objective of achieving reliable performance despite the scarcity of labeled data and log instability.  

% In literature, RCA is one of the preferred applications of agentic AI, as proven by the abundant work available for examination. The reason for this lies with the complexity of RCA in modern infrastructures, requiring On Call Engineers (OCE) to manually distill relevant information from voluminous and heterogeneous sources such as system logs, incident reports, and metrics in short time frames, thus requiring significant proficiency and experience.
% With RCACopilot~\cite{chen2024automatic}, Chen et al. propose an LLM-based solution to assist OCEs in performing such challenging operations, enabling them to define automatic workflows for data collection, analysis, and report generation.
% In the same context, Zhang et al.~\cite{zhang2024automated} conduct an extensive evaluation on automated RCA with GPT-4, proving that in-context learning can yield strong performance, without resorting to expensive fine-tuning operations.
% Other examples can be found in~\cite{wang2024rcagent} and ~\cite{roy2024exploring}, both being studies of solutions, namely RCAgent and ReAct, fully embodying the tool-enhanced agentic paradigm for practical RCA, showing the feasibility of the approach in real-world scenarios.
% TAMO~\cite{zhang2025tamo} is yet another fully agentic solution for RCA, with the specific aim to tackle the challenges stemming from the multi-modal nature of cloud-native data. 
% Xiang et al.~\cite{xiang2025simplifying} integrate graph-based databases, collecting Kubernetes cluster state information, with LLMs for automatic generation of RCA reports, and demonstrate practical application of the solution.
% On the side of incident mitigation, systems such as LLexus~\cite{las2024llexus} and Nissist~\cite{an2024nissist} are proposed as agentic assistants for recommending and even executing mitigation plans given Troubleshooting Guides and incident history, which are generally unstructured, incomplete data that needs to be manually examined by OCEs. 
RCA is a prominent application of agentic AI because of the complexity of modern infrastructures, where On-Call Engineers (OCEs) must rapidly synthesize heterogeneous data from logs, metrics, and reports. Chen et al.~\cite{chen2024automatic} propose RCACopilot, an LLM-based system that automates data collection, analysis, and report generation. Similarly, Zhang et al.~\cite{zhang2024automated} show that GPT-4 with in-context learning achieves strong automated RCA performance without fine-tuning. Other agentic solutions, including RCAgent and ReAct~\cite{wang2024rcagent,roy2024exploring}, demonstrate tool-augmented reasoning for practical RCA, while TAMO~\cite{zhang2025tamo} addresses multimodal cloud-native data. Xiang et al.~\cite{xiang2025simplifying} combine graph databases with LLMs to generate RCA reports from Kubernetes cluster states.
Engineers at Meta~\cite{hsu2024leveraging} introduced an AI-assisted root cause analysis system developed to streamline incident response in complex environments like their web monorepo, with the strategic application of large language models to perform ranking of potential root causes.
Beyond diagnosis, systems such as LLexus and Nissist~\cite{las2024llexus,an2024nissist} extend the agentic paradigm to incident mitigation by recommending or executing remediation plans from unstructured troubleshooting data.


%Beyond failure management, LLM agents also found usage in the industry as natural language, conversational interfaces to complex systems and intelligent assistants for human operators, lowering the skill barrier required of the latter~\cite{yunjchoi2023conversational, kapoor2025aiops}. In this context, renewed interest has emerged in the intent-based paradigm, wherein operators specify a desired system state, potentially expressed in natural language, and delegate the realization of that state to AI-driven agents. Recent studies demonstrate the applicability of this paradigm across multiple layers of cloud-native infrastructure. Prior work has explored the automatic generation of Kubernetes configuration artifacts from high-level specifications~\cite{dubey2024leveraging, sacco2025intent}, as well as end-to-end enforcement of intent in broader cloud configuration management scenarios~\cite{brodimas2025intent, mekrache2024intent}, illustrating the feasibility of leveraging AI techniques to bridge the gap between human-defined objectives and executable infrastructure policies.

%Another prominent use case for agentic AI lies in security operations, considering that the evolution of cloud applications into highly dynamic systems has made traditional techniques more challenging to apply. In light of this, Cao et al.~\cite{cao2024llm} devise LLM-CloudSec, a framework for timely, fine-grained vulnerability detection and analysis in cloud applications. Agentic AI, however, may at the same time represent a tool for securing operations but also give birth to new vulnerabilities: Pasquini et al.~\cite{pasquini2025aiops} reflect on the security risks of adopting AIOps systems, identifying these autonomous, telemetry-driven workflows as potential attack vectors, and delineate a solution for remediation.

Beyond failure management, LLM-based agents increasingly serve as conversational interfaces and intelligent assistants for cloud operations, lowering the expertise barrier and enabling intent-based workflows in which operators specify desired system states in natural language and delegate their realization to AI-driven agents~\cite{yunjchoi2023conversational,kapoor2025aiops}. This paradigm has been applied to automated Kubernetes manifest generation and broader cloud configuration enforcement, demonstrating how agentic systems translate high-level objectives into executable infrastructure policies~\cite{dubey2024leveraging,sacco2025intent,brodimas2025intent,mekrache2024intent}. In parallel, agentic AI is gaining traction in security operations, where the dynamism of cloud environments challenges traditional methods: frameworks such as NVIDIA Morpheus~\cite{huang_nandakumar2024augmenting} and LLM-CloudSec~\cite{cao2024llm} respectively enable automatic alert triage following into generation of actionable insights from raw data, and fine-grained vulnerability detection and analysis, while recent work also highlights the security risks introduced by autonomous, telemetry-driven AIOps pipelines and proposes mitigation strategies~\cite{pasquini2025aiops}.

%More applications of LLM agents reside in DevOps, where optimizing management processes can grant several benefits. Tasks in this field, consisting of predictive maintenance, resource allocation optimization, monitoring, load balancing, and workflow automation, are tackled in \cite{bokkena2024optimizing}, where the author evaluates the impact of delegating their completion to LLMs using the Google Cluster Usage trace as a benchmark base. 
%Our contribution to the AIOps landscape lies in a different use case from those commonly found in literature: while \Solution{} is similarly intended as an assistant for intelligent operations, its primary purpose is neither incident management nor security analysis, but rather application-aware resource scaling and optimization in Kubernetes clusters, delivered in an online, fully functioning platform.
LLM agents are also being explored in DevOps to optimize operational processes, including predictive maintenance, resource allocation, monitoring, load balancing, and workflow automation. For example,~\cite{bokkena2024optimizing} evaluates delegating these tasks to LLMs using the Google Cluster Usage trace, focusing on performance gains in cluster management workflows.

In contrast to prior work - largely centered on incident management, security, or offline performance evaluation - \Solution\ targets application-aware resource scaling and optimization in K8s clusters. It operates as an online, fully functional system that continuously adapts resource allocation based on application-level context, positioning it as a proactive optimization framework rather than a reactive operational assistant.


\section{System Model}
\label{sec:system}

\subsection{\Solution\ Overview}
\pn{AutoSage runs outside the Kubernetes cluster and connects using normal kubeconfig access.It reads cluster metrics, predicts near-future load, and asks the LLM for a scaling choice.Then MCDA checks that choice and can correct it if needed.The final action is sent to Kubernetes through kubectl patch/scale commands.After that, Kubernetes itself applies the change through its normal control-plane reconciliation.Figure 1 shows this full flow from data collection to decision, validation, execution, and dashboard feedback.}

Agentic AIOps represents the shift from passive observability to autonomous operational intelligence, by introducing goal-driven, decision-making agents that can interpret system state, reason about potential actions, and safely execute changes in production environments. 
In Kubernetes systems, Agentic AI systems can continuously analyze telemetry, predict capacity, and take corrective actions such as scaling workloads, while adhering to guardrails. 

In this section, we present our entry in this emerging panorama: \Solution{}, a solution for intelligent, context-aware auto-scaling operations in containerized infrastructures.
\Solution{} is a modular platform that operates \emph{outside} the managed
K8s cluster.  It authenticates via standard \texttt{kubeconfig}
credentials, collects telemetry through the Kubernetes Metrics API, and applies
scaling actions with \texttt{kubectl patch} commands.  Figure~\ref{fig:arch}
shows the five principal layers, described in detail below.

% ---- Architecture diagram ----
\begin{figure*}[t]
\centering
\begin{tikzpicture}[
    sub/.style={draw, rounded corners=2pt, fill=#1, font=\scriptsize,
                align=center, inner sep=3pt, minimum width=2.4cm,
                minimum height=0.8cm},
    sub/.default={gray!10},
    arrow/.style={-{Stealth[length=5pt]}, thick},
    note/.style={font=\tiny\itshape, text=gray!70},
    title/.style={font=\small\bfseries}
]

% ---- Row 1: Kubernetes Cluster ----
\node[sub=blue!15]  (msrv) at (0,0)    {Metrics\\Server};
\node[sub=blue!15]  (klet) at (3,0)    {Kubelet};
\node[sub=blue!15]  (apis) at (6,0)    {API Server};
\draw[rounded corners=5pt, fill=blue!4, draw=blue!40]
      (-1.5,-0.7) rectangle (7.5,0.7);
\node[title] at (1.6,1.05) {Kubernetes Cluster};
% redraw sub-nodes on top
\node[sub=blue!15]  at (0,0)    {Metrics\\Server};
\node[sub=blue!15]  at (3,0)    {Kubelet};
\node[sub=blue!15]  at (6,0)    {API Server};

% ---- Row 2: Metrics Collection ----
\node[sub=green!20] (ktop) at (0,2.5)  {\texttt{kubectl top}\\(primary)};
\node[sub=green!20] (mapi) at (3,2.5)  {Metrics API\\(\texttt{v1beta1})};
\node[sub=green!20] (ksum) at (6,2.5)  {Kubelet\\Summary};
\draw[rounded corners=5pt, fill=green!4, draw=green!40]
      (-1.5,1.8) rectangle (7.5,3.2);
\node[title] at (1.8,3.55) {Metrics Collector};
\node[sub=green!20] at (0,2.5)  {\texttt{kubectl top}\\(primary)};
\node[sub=green!20] at (3,2.5)  {Metrics API\\(\texttt{v1beta1})};
\node[sub=green!20] at (6,2.5)  {Kubelet\\Summary};
% fallback arrows
\draw[-{Stealth[length=3pt]}, thin, gray] (1.25,2.5) -- (1.75,2.5);
\draw[-{Stealth[length=3pt]}, thin, gray] (4.25,2.5) -- (4.75,2.5);

% ---- Row 3: Predictive Monitoring ----
\node[sub=orange!20] (fore) at (0,5)   {Trend +\\Seasonal};
\node[sub=orange!20] (uq)   at (3,5)   {Bootstrap UQ\\($B{=}50$)};
\node[sub=orange!20] (anom) at (6,5)   {Isolation Forest\\+ Platt};
\draw[rounded corners=5pt, fill=orange!4, draw=orange!40]
      (-1.5,4.3) rectangle (7.5,5.7);
\node[title] at (1.4,6.05) {Predictive Monitoring};
\node[sub=orange!20] at (0,5)   {Trend +\\Seasonal};
\node[sub=orange!20] at (3,5)   {Bootstrap UQ\\($B{=}50$)};
\node[sub=orange!20] at (6,5)   {Isolation Forest\\+ Platt};

% ---- Row 4: LLM Advisor (two sub-rows) ----
\draw[rounded corners=5pt, fill=red!3, draw=red!30]
      (-1.5,6.8) rectangle (7.5,9.5);
\node[title] at (1.2,9.85) {LLM Autoscaling Advisor};
% sub-row A: models + detection
\node[sub=red!15]    (qwen)  at (0,8.7)   {Qwen\\(primary)};
\node[sub=red!15]    (groq)  at (3,8.7)   {Groq / Llama\\(fallback)};
\node[sub=yellow!30] (sdet)  at (6,8.7)   {Statefulness\\Detection};
% sub-row B: MCDA + enforcement
\node[sub=violet!15, minimum width=3.2cm] (mcda) at (1.5,7.5)
      {MCDA / TOPSIS\\Cross-Validation};
\node[sub=yellow!30, minimum width=3.2cm] (enf)  at (5.2,7.5)
      {HPA / VPA\\Enforcement};
% fallback arrow
\draw[-{Stealth[length=3pt]}, thin, gray] (1.25,8.7) -- (1.75,8.7);
% internal flows
\draw[arrow, thin, gray!60] (qwen.south)  -- (mcda.north west);
\draw[arrow, thin, gray!60] (groq.south)  -- (mcda.north);
\draw[arrow, thin, gray!60] (sdet.south)  -- (enf.north east);
\draw[arrow, thin, gray!60] (mcda.east)   -- (enf.west);

% ---- Row 5: Dashboard ----
\node[sub=purple!15] (rest) at (0,11.2)    {REST API};
\node[sub=purple!15] (ws)   at (2.3,11.2)  {WebSocket};
\node[sub=purple!15] (uiv)  at (4.5,11.2)  {UQ Viz};
\node[sub=purple!15] (mcv)  at (6.5,11.2)  {MCDA\\Card};
\draw[rounded corners=5pt, fill=purple!4, draw=purple!30]
      (-1.5,10.5) rectangle (7.5,11.9);
\node[title] at (3,12.25) {Web Dashboard \& API};
\node[sub=purple!15] at (0,11.2)    {REST API};
\node[sub=purple!15] at (2.3,11.2)  {WebSocket};
\node[sub=purple!15] at (4.5,11.2)  {UQ Viz};
\node[sub=purple!15] at (6.5,11.2)  {MCDA\\Card};

% ---- Arrows between layers ----
\draw[arrow] (3,0.7)   -- node[right, note, pos=0.5] {every 300\,s} (3,1.8);
\draw[arrow] (3,3.2)   -- node[right, note, pos=0.5] {CPU\%, Mem\%, pods}
                           (3,4.3);
\draw[arrow] (3,5.7)   -- node[right, note, pos=0.5]
                           {forecasts + UQ + anomalies} (3,6.8);
\draw[arrow] (3,9.5)   -- node[right, note, pos=0.5]
                           {recommendation + validation} (3,10.5);

% ---- Actuation return arrow ----
\draw[arrow, dashed] (-1.5,8.1) -- (-2.2,8.1) |- node[left, note, pos=0.2]
      {\texttt{kubectl patch}} (-1.5,0);

\end{tikzpicture}
\Description{Detailed architecture of AutoSage with five layers, each
containing separate sub-component boxes.  Kubernetes Cluster has Metrics
Server, Kubelet, API Server.  Metrics Collection has three fallback methods
connected by arrows.  Predictive Monitoring has forecasting, bootstrap UQ,
and anomaly detection.  LLM Advisor has two rows: Qwen/Groq with
statefulness detection, and MCDA/TOPSIS with HPA/VPA enforcement, connected
by internal flow arrows.  Web Dashboard has REST, WebSocket, UQ Viz, MCDA
Card.  Layer arrows show data flow; dashed arrow shows actuation return.}
\caption{Architecture of \Solution{}.  Each layer contains its internal
components as separate modules.  Solid arrows show the data flow; the dashed
arrow shows the actuation path.  Small gray arrows within layers indicate
internal flows (e.g., \ LLM fallback chain, MCDA feeding enforcement). \as{In k8s cluster we miss kubelet top right?}} \pn{if you mean kubectl top then we should not put kubectl top inside the Kubernetes Cluster layer.kubectl top is a client command/tool (outside the cluster),it queries cluster metrics through the API path (Metrics Server / API Server),so it belongs to the Metrics Collection layer, not inside cluster internals.}
\label{fig:arch}
\end{figure*}

\subsection{Metrics Collector}
\label{sec:metrics}

The \emph{Metrics Collector} gathers real-time resource telemetry from the target K8s cluster.  Rather than requiring a full monitoring stack such as Prometheus, \Solution{} queries only a lightweight Kubernetes Metrics Server~\cite{k8s_metrics_server,k8s_resource_metrics_pipeline,k8s_kubectl_top}, a scalable and efficient plugin that operates a source of container resource metrics for autoscaling pipelines.
In such a way, our approach works with any standard cluster, eliminating the need to deploy additional infrastructure.  
The collector retrieves metrics using a prioritized, sequential strategy. It first attempts to obtain metrics using a primary method. If this attempt fails or does not return within a fixed timeout ($10$\,s), the system proceeds to the next method in the predefined order. This process continues until metrics are successfully retrieved or all available methods have been attempted. 
%Each individual metrics request is allowed to run for at most 10\,s. 
Individual requests that exceed this timeout are treated as failed, triggering the next method in the sequence. This design ensures predictable latency while providing robustness against transient failures or slow responses from individual metric sources.

The collector attempts to retrieve metrics from the following sources, in order of priority:
\begin{enumerate}
    \item \textbf{Kubectl top.}  The primary method executes \texttt{kubectl top nodes} as a subprocess and parses the tabular output for per-node CPU (millicores) and memory (MiB).  This service internally reads from the Metrics Server but returns values in a format similar to the traditional \texttt{top} command. We have empirically observed that on a single-node testbed, this call completes in ${\sim}200$--$500$\,ms, which makes our timeout value a valid threshold.
    %\pn{In our single-node CrownLabs testbed, this call typically completed in a few hundred milliseconds (empirical observation).}
    \item \textbf{Metrics API.}  If \texttt{kubectl top} fails or is unavailable, the collector queries via direct API requests to the Metrics Server. The endpoint \texttt{metrics.\allowbreak{}k8s.\allowbreak{}io/v1beta1} is accessed via a Kubernetes Python client, %(\texttt{CustomObjectsApi}), 
    converting raw millicore and byte values to percentages using each node's allocatable capacity. 
    \item \textbf{Kubelet Summary API.}  As a last resort, the collector queries each node's Kubelet via the API server proxy that returns \texttt{usageNanoCores}. This value represents how much CPU a workload is using, thus we convert it to CPU usage using the formula 1\,CPU\,$=$\,$10^{9}$\,nanocores.
\end{enumerate}

The methods are not executed in parallel since only one method is active per collection cycle; the fallback is transparent to downstream consumers.  Raw values are normalized to cluster-wide utilization percentages: $\text{CPU\%} = \text{usage} / \text{allocatable} \times 100$, and analogously for memory.  
The \emph{Metrics Collector} generates an aggregated snapshot consisting of: CPU\%, memory\%, pod count \as{what is this? no explanation, not all are familiar with k8s} \pn{Pod count is the number of pods currently running for the monitored workload/namespace. It gives a direct view of how many application instances are active.}, node count \as{what is this? no explanation} \pn{Node count is the number of Kubernetes worker machines currently available in the cluster. It indicates cluster capacity and helps normalize resource usage.},
network~I/O \as{what is this? no explanation} \pn{Network I/O is the amount of data sent and received over the network by the monitored workloads/nodes (input/output traffic). It is useful to detect communication-heavy load conditions.}, and disk~I/O \as{what is this? no explanation} \pn{Disk I/O is the amount of data read from and written to storage by workloads/nodes. It captures storage pressure that may not appear in CPU or memory alone.}. With a time interval $\Delta t$, the module generates a matrix \as{Is it a matrix or array?} \pn{It is an array (list) of snapshots in the implementation.
You can think of it like a matrix when explaining it, but technically it is stored as a rolling list.}using the last $N$ samples in a rolling in-memory window to feed the forecasting module. By default, we set = $\Delta t =300$\,s but this value can be customized for faster or slower decisions.

\subsection{Predictive Monitoring}
\label{sec:prediction}

\subsubsection{Time-Series Forecasting.}
Using the collected metrics, we can now predict future usage of CPU and memory using two different approaches.
The \textit{CPU utilization} is forecast over a horizon of $H = 6$~hours using a composite model: as a weighted combination of a linear trend component and a seasonal component capturing daily periodicity. %A first-order polynomial (linear trend) is fitted to the last~$N$ samples via least-squares, (\texttt{numpy.polyfit}) and a seasonal component capturing daily periodicity is added, resulting in a final formula as:
In particular, we assume the time series can be decomposed into two components: trend and seasonality.
Formally, the $h$-step-ahead prediction is given by:
\begin{equation}\label{eq:forecast}
  \hat{y}(t+h) \;=\; 0.7 \cdot f_{\mathrm{trend}}(t+h)
                 \;+\; 0.3 \cdot f_{\mathrm{season}}(t+h),
  \quad h = 1, \ldots, H
\end{equation}
where $f_{\mathrm{trend}}$ is the linear extrapolation and $f_{\mathrm{season}}$ is the corresponding value from the previous daily cycle. 
The trend component $f_{\mathrm{trend}}$ is modeled as a first-order polynomial:
\begin{equation}\label{eq:trend}
  f_{\mathrm{trend}} = a t + b,
\end{equation}
where the coefficients $a$ and $b$ are estimated via ordinary least squares using the most recent $N$ observations. This local linear regression captures the short-term evolution of the underlying signal and allows the model to adapt to recent changes in level and slope.

The seasonal component $f_{\mathrm{season}}$ captures systematic daily periodic effects and accounts for recurring intra-day patterns not explained by the linear trend. It models deterministic cyclic behavior with a fixed period corresponding to one day.

The final forecast is obtained as a convex combination of the two components, assigning weights of $0.7$ to the trend and $0.3$ to the seasonal term. This weighting emphasizes the dominant role of the recent linear dynamics while still incorporating structured periodic fluctuations. The CPU forecast can be interpreted as a simple additive decomposition with fixed combination weights, designed to capture both short-term trend behavior and stable daily seasonality.


\textit{Memory utilization}, instead, is forecast with a separate exponential smoothing model with smoothing parameter $\alpha = 0.3$. The one-step-ahead forecast is computed as:
\begin{equation}\label{eq:mem_smoothing}
  \hat{m}_{t+1} = \alpha m_t + (1-\alpha)\hat{m}_{t},
\end{equation}
where $m_t$ is the observed memory utilization at time $t$ and $\hat{m}_t$ represents the smoothed estimate obtained at the previous time step.

This formulation updates the forecast recursively by combining the most recent observation with the prior smoothed value. The parameter $\alpha$ controls the relative weight assigned to new information versus historical estimates, with $\alpha = 0.3$ placing greater emphasis on past observations while still allowing the model to adapt to recent changes.


\subsubsection{Bootstrap Uncertainty Quantification.}
\label{subsec:uncert}

Point forecasts alone cannot express how much trust an operator should place in a prediction.  \Solution{} therefore wraps every forecast in a \emph{prediction interval} produced by bootstrap resampling~\cite{efron1993introduction}.

Given $N$ historical observations, uncertainty is quantified via a nonparametric bootstrap procedure. Specifically, $B = 50$ bootstrap samples, each of size $N$, are drawn with replacement from the original dataset. For each bootstrap replicate $b \in \{1,\dots,B\}$, a linear model is fitted and $h$-step-ahead predictions $\hat{y}^{(b)}(t+h)$ are generated.

The epistemic (model) uncertainty at forecast horizon $h$ is estimated as the empirical variance across bootstrap predictions,
\begin{equation}\label{eq:epistemic} \sigma^2_{\mathrm{epi}}(h) = \frac{1}{B}\sum_{b=1}^{B} \bigl(\hat{y}^{(b)}(t+h) - \bar{y}(t+h)\bigr)^{2}, 
\end{equation}
where $\bar{y}(t+h) = \frac{1}{B}\sum_{b=1}^{B}\hat{y}^{(b)}(t+h)$ denotes the mean bootstrap forecast. This quantity captures variability due to finite-sample effects and parameter uncertainty.

The total predictive variance is assumed to increase with the forecast horizon and is modeled as:
\begin{equation}\label{eq:total_var}
  \sigma^2(h) = \sigma^2_{\mathrm{ale}} \cdot \sqrt{h}
              + \sigma^2_{\mathrm{epi}} \cdot h,
\end{equation}
This formulation reflects the intuition that both observational noise and model uncertainty accumulate over time.

Assuming approximate normality of the predictive distribution, a $100(1-\alpha)\%$ prediction interval is constructed as
\[
  \hat{y}(t+h) \pm z_{\alpha/2}\,\sigma(h),
\]
where $z_{\alpha/2}$ is the standard normal quantile (e.g., $z_{0.025}=1.96$ for a $95\%$ prediction interval).

In addition, the system computes four \emph{exceedance probabilities} in the form of:
\[
  \Pr\!\left(Y(t+h) > \tau\right)
\]
for predefined utilization thresholds $\tau \in \{50,70,80,90\}\%$. These probabilities are obtained using the complementary cumulative distribution function of the normal distribution with mean $\hat{y}(t+h)$ and variance $\sigma^2(h)$. %They quantify the risk that the predicted value exceeds operationally relevant capacity limits and can be used for proactive alerting or resource management decisions.
These values directly reflect operational risk (e.g., the probability of entering high- or critical-utilization zones) and are used for dashboard visualizations, proactive alerting, and scaling decisions.

\pn{The prediction interval gives a plausible range for future utilization (not just one point estimate).
Then, exceedance probability estimates how likely utilization is to cross practical thresholds (50/70/80/90%).
These probabilities are used by the dashboard and autoscaling logic: higher exceedance at high thresholds means higher risk and can trigger stronger scale-up recommendations, while low exceedance supports maintain/scale-down decisions. or we can say it like  this The interval \([\hat{y}(t+h)\pm z_{\alpha/2}\sigma(h)]\) provides the expected range of future utilization at confidence \(100(1-\alpha)\%\), so operators can see forecast uncertainty instead of only a point estimate.}

\subsubsection{Calibrated Anomaly Detection.}
An Isolation Forest~\cite{liu2008isolation} with contamination
$c = 0.1$ produces a raw anomaly score $s \in [-1,1]$.  Since raw scores are
not directly interpretable as probabilities, \Solution{} applies
\emph{Platt scaling}~\cite{platt1999probabilistic}:
\begin{equation}\label{eq:platt}
  \Pr(\text{anomaly} \mid s) = \sigma(As + B)
    = \frac{1}{1 + e^{-(As + B)}},
\end{equation}
with learned parameters $A = -5.0$ and $B = -0.5$.  The calibrated probability
is mapped to a severity distribution across four levels (low, medium, high,
critical), and a detection confidence is computed as
$\gamma = 2 \, |P(\text{anomaly}) - 0.5|$. \as{what is this? how is it used?} \as{We need to expan a bit more here} \pn{This block converts anomaly detection from a binary flag into a risk score that operators can act on.
Isolation Forest gives a raw anomaly score, but raw scores are hard to interpret.
Platt scaling transforms that score into an estimated probability of anomaly (0 to 1), which is easier to understand and compare over time.
That probability is then mapped to severity levels (low/medium/high/critical) for dashboard display and alert prioritization.
The confidence term 
γ=2∣P(anomaly)−0.5∣ γ=2∣P(anomaly)−0.5∣ measures how far the probability is from uncertainty (0.5): values near 1 mean strong confidence, values near 0 mean uncertain detection.
In practice, these outputs are used to trigger warnings, explain risk in the UI, and provide additional context to scaling decisions.”
}

\subsection{LLM Autoscaling Advisor}
\label{sec:llm}

The autoscaling advisor casts every scaling decision as a natural-language
reasoning problem.  A structured prompt is assembled from four sources of
context: (i)~current metrics (CPU/memory utilisation, pod count,
ready/available pods), (ii)~forecasts (current value, peak, trend, per-hour
predictions for $H = 6$~hours), (iii)~deployment state (replicas, min/max
bounds, resource requests/limits, HPA/VPA status), and (iv)~statefulness
classification (see below).  The LLM returns a JSON object specifying the
recommended \texttt{action} (\texttt{scale\_up}, \texttt{scale\_down}, or
\texttt{maintain}), a \texttt{scaling\_type} (\texttt{hpa} or \texttt{vpa}),
target replicas or VPA resource requests, a confidence score, and a free-text
rationale.

\subsubsection{Model Selection and Caching.}
\Solution{} implements a two-tier LLM fallback chain.  The primary model is a
self-hosted open-source LLM (e.g.\ Qwen) accessed via an OpenAI-compatible
endpoint, with a timeout of 240\,s and generation temperature $\theta = 0.1$.
If the primary model is unavailable, the system falls back to the Groq cloud
API (Llama~3.1-8B, timeout 15\,s).  To limit API costs, recommendations are
cached for a configurable TTL (default 300\,s) and calls are rate-limited to a
minimum interval of 30\,s.

\subsubsection{HPA vs.\ VPA Decision.}\label{sec:hpa-vpa}
Choosing between horizontal and vertical scaling is fundamental: adding
replicas (HPA) to a stateful application that stores session data inside the
pod would cause data loss, while increasing per-pod resources (VPA) for a
stateless web server wastes capacity that could be spread across replicas.
\Solution{} determines the scaling type through a three-stage process.

\paragraph{Stage~1: Statefulness Detection.}
Before calling the LLM, the system inspects the Kubernetes deployment object
and classifies the workload's state management through five signals, evaluated
in priority order:
\begin{enumerate}
  \item \textbf{Annotation}
        (\texttt{ai4k8s.io/\allowbreak{}state\--management}):
        explicit operator declaration (\emph{high} confidence).
  \item \textbf{Environment variables}: presence of \texttt{REDIS},
        \texttt{DATABASE}, \texttt{POSTGRES} indicates stateless (external
        state); \texttt{LOCAL\_STORAGE}, \texttt{PERSISTENT} indicates
        stateful (\emph{medium} confidence).
  \item \textbf{Persistent volumes}:
        \texttt{PersistentVolumeClaim} or \texttt{hostPath} mounts indicate
        stateful (\emph{high} confidence); \texttt{emptyDir} is ignored.
  \item \textbf{Namespace services}: presence of Redis, database, or cache
        services in the same namespace indicates stateless (\emph{medium}).
  \item \textbf{Labels}: standard Kubernetes labels provide additional hints
        (\emph{low} confidence).
\end{enumerate}
The detected type and confidence are included in the LLM prompt.

\paragraph{Stage~2: LLM Reasoning.}
The LLM receives the statefulness classification as part of the prompt and
returns a \texttt{scaling\_type} field.  Because the LLM has access to the full
deployment context, it can make nuanced decisions---for example, recommending
VPA when a stateless application is already at maximum replicas.

\paragraph{Stage~3: Post-Processing Enforcement.}
LLM outputs can be inconsistent: the reasoning text may mention ``state stored
inside the pod'' while the JSON sets \texttt{scaling\_type:~hpa}.  A
rule-based enforcement layer catches such contradictions:
\begin{itemize}
  \item If the reasoning contains stateful indicators but the output is HPA, the system corrects to VPA and computes the target CPU/memory from the current usage plus a 20\% headroom.
  \item If the detection classifies the workload as stateless but the LLM outputs VPA, the system forces HPA and recalculates target replicas from current metrics and forecast peaks.
  \item If no statefulness information is available and the LLM chose HPA,
        the system defaults to VPA as the safer option.
\end{itemize}

\subsubsection{Scaling Actuation.}
Once the scaling type and targets are finalised, the corresponding engine module applies the change:
\begin{itemize}
  \item \textbf{HPA:} a JSON Patch sets \texttt{minReplicas} and
        \texttt{maxReplicas} on the HPA resource
        (\texttt{kubectl patch hpa --type=json}).
  \item \textbf{VPA:} a strategic merge patch updates the container's \\
        \texttt{resources.requests} in the Deployment spec
        (\texttt{kubectl patch deployment --type=merge}).
\end{itemize}

\subsection{MCDA Cross-Validation}\label{sec:mcda}

LLM recommendations are inherently non-deterministic.  To provide a formal safeguard, every recommendation is cross-validated by a Multi-Criteria Decision Analysis (MCDA) module that implements the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS)~\cite{hwang1981topsis}.

\subsubsection{Alternative Generation.}
From the current replica count~$r$, the module generates $5$--$8$ candidate
alternatives: $r + \delta$ for $\delta \in \{-2, -1, 0, +1, +2, +3\}$, the
minimum and maximum replicas, and an ``ideal'' replica count that would bring
CPU utilisation to~70\%.  Candidates outside the $[\textit{min},\textit{max}]$
range are discarded.

\subsubsection{Criteria Evaluation.}
Each alternative is scored on five criteria (Table~\ref{tab:criteria}).  Scores
are normalised, weighted, and processed by the TOPSIS algorithm.

\begin{table}[t]
\caption{MCDA criteria and their evaluation.}
\label{tab:criteria}
\centering
\small
\begin{tabular}{@{}llp{3.8cm}@{}}
\toprule
\textbf{Criterion} & \textbf{Type} & \textbf{Description} \\
\midrule
Cost            & Cost    & Normalised replica count $r / r_{\max}$ \\
Performance     & Benefit & Proximity of estimated CPU to 70\% sweet spot \\
Stability       & Benefit & Penalises large replica changes and contra-trend scaling \\
Forecast Align. & Benefit & Estimated peak CPU in 50--75\% comfort zone \\
Response Time   & Cost    & Estimated CPU as proxy for latency \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{TOPSIS Ranking.}
Let $\mathbf{X} \in \mathbb{R}^{m \times n}$ be the decision matrix with $m$
alternatives and $n = 5$ criteria.  TOPSIS proceeds as follows:
\begin{enumerate}
  \item \textbf{Normalise:}
        $r_{ij} = x_{ij} \big/ \sqrt{\sum_{i=1}^{m} x_{ij}^2}$.
  \item \textbf{Weight:}
        $v_{ij} = w_j \cdot r_{ij}$, where $\mathbf{w}$ is one of four
        predefined weight profiles (balanced, performance-first,
        cost-optimised, stability-first).
  \item \textbf{Ideal / Anti-ideal:}
        $A^{+}_j = \max_i v_{ij}$ (benefit) or $\min_i v_{ij}$ (cost);
        $A^{-}_j$ vice versa.
  \item \textbf{Distances:}
        $D^{+}_i = \lVert \mathbf{v}_i - A^{+} \rVert_2$, \;
        $D^{-}_i = \lVert \mathbf{v}_i - A^{-} \rVert_2$.
  \item \textbf{Closeness:}
        $C_i = D^{-}_i / (D^{+}_i + D^{-}_i) \in [0,1]$.
\end{enumerate}
The alternative with the highest $C_i$ is the MCDA-optimal action.

\subsubsection{Agreement and Override.}
The MCDA module compares the LLM's recommended target~$r_L$ with the
MCDA-optimal target~$r_M$.  Agreement is classified into three levels:
\begin{itemize}
  \item \textbf{Full agreement:} same scaling direction and
        $|r_L - r_M| \leq 1$.
  \item \textbf{Partial agreement:} same direction, different magnitude.
  \item \textbf{Divergence:} opposite scaling directions.
\end{itemize}
If partial or divergent and the MCDA closeness score exceeds the LLM's by more
than a threshold $\delta = 0.15$, the system overrides the LLM recommendation
with the MCDA-optimal action.  The full validation result---agreement level,
both scores, the dominance margin, criteria weights, and ranking---is attached
to the recommendation and displayed on the dashboard.

\subsubsection{Weight Profiles.}
Four weight profiles allow operators to express different priorities
(Table~\ref{tab:weights}).  The default profile is \emph{balanced}.

\begin{table}[t]
\caption{MCDA weight profiles ($w_j$ sums to 1.0).}
\label{tab:weights}
\centering
\small
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Profile} & \textbf{Cost} & \textbf{Perf.} & \textbf{Stab.}
                  & \textbf{Forec.} & \textbf{Resp.} \\
\midrule
Balanced           & 0.20 & 0.30 & 0.25 & 0.15 & 0.10 \\
Performance-first  & 0.10 & 0.40 & 0.15 & 0.20 & 0.15 \\
Cost-optimised     & 0.40 & 0.20 & 0.20 & 0.10 & 0.10 \\
Stability-first    & 0.15 & 0.20 & 0.40 & 0.15 & 0.10 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Dashboard and Actuation}\label{sec:dashboard}

% \Solution{} exposes a Flask-based web application that serves both REST
% endpoints and WebSocket channels (via Flask-SocketIO).  The monitoring
% dashboard renders:
% \begin{itemize}
%   \item Prediction intervals that widen with the forecast horizon, providing a
%         visual cue of uncertainty growth.
%   \item Exceedance probability bars for four utilization thresholds.
%   \item Stacked bars decomposing total uncertainty into aleatoric and
%         epistemic components.
%   \item Calibrated anomaly probability with severity distribution.
%   \item MCDA validation cards showing agreement level, TOPSIS scores, and
%         ranking.
% \end{itemize}
% Scaling recommendations are generated asynchronously; the frontend polls via
% WebSocket for status updates and displays a progress indicator.  When an
% operator approves a recommendation, the dashboard triggers a
% \texttt{kubectl~patch} command through the appropriate engine module
% (\texttt{autoscaling\_engine} for HPA, \texttt{vpa\_engine} for VPA), closing
% the actuation loop.

\Solution{} exposes a web-based control plane that provides both REST APIs and WebSocket channels for real-time interaction via Flask and Flask-SocketIO. The monitoring dashboard presents: \textit{(i)} prediction intervals that expand with the forecast horizon to reflect uncertainty growth; \textit{(ii)} the four exceedance probabilities across the utilization thresholds $(50, 70, 80, 90)$ as mentioned in Section~\ref{subsec:uncert}; 
% because the system computes probabilities for thresholds 50 70 80 90 percent they are defined earlier in the predictive/UQ part.
\textit{(iii)} where prediction uncertainty comes from (data noise vs model uncertainty), shown as the data-noise part from Eq.~(\ref{eq:total_var}) \as{not sure if this is the right eq}\pn{Yes, this is correct: Eq.~(\ref{eq:total_var}) (currently Eq.(5)) is used for the data-noise part, and Eq.~(\ref{eq:epistemic}) is used for model uncertainty.} and the model-uncertainty part from Eq.~(\ref{eq:epistemic}); 
\textit{(iv)} calibrated anomaly likelihood with associated severity; 
and \textit{(v)} MCDA-based validation summaries including agreement levels, TOPSIS scores, and ranking.

Scaling recommendations are computed asynchronously and streamed to the frontend with progress updates. Upon operator approval, the system actuates the selected policy (via \texttt{kubectl~patch} command) by invoking the appropriate autoscaling module (HPA or VPA), thereby closing the monitoring–analysis–decision–actuation loop.

\section{Evaluation}
\label{sec:evaluation}

This section reports a live comparison between Kubernetes-native autoscaling
and \Solution{}, with emphasis on timing, service-level behavior, and resource
footprint under the same workload conditions.

\subsection{Comparison with Native HPA/VPA}

To quantify the practical behavior of \Solution{} against Kubernetes-native
autoscalers, we implemented and executed a dedicated benchmark runner.
%(\texttt{autoscaling\_native\_comparison.py}) in the project repository.
The experiment was run on the CrownLabs deployment used throughout this work.% on the \texttt{test-app-autoscaling} workload in namespace \texttt{ai4k8s-test}. 
For fairness, each method started from the same baseline
(\(2\) replicas) before load was applied.

Load was generated in two ways: (i) increasing background CPU pressure by
setting \texttt{CPU\_LOAD\_PERCENT=95} in the target deployment, and
(ii) issuing burst requests to \texttt{/cpu-load} from an ephemeral in-cluster
\texttt{curl} pod. We evaluate \Solution{}'s predictive auto-scaling loop against native HPA and native VPA with a per-method
timeout of \(60\) seconds \cz{@PEDRAM: what is this timeout? benchmark duration? Should be better described.}\pn{We can describe it simply as: ``This timeout is the per-method benchmark observation duration in each run (60s). During this window, we monitor whether a method emits a decision/recommendation and whether the first change is applied. It is not an API-server provisioning timeout.''}. To make timing semantics explicit, we
separate three phases in the loop: (i) \emph{control-plane provisioning}, object create/update latency in the API server, (ii) \emph{decision/reaction}, the time until a scaling recommendation or controller decision appears, and
(iii) \emph{post-decision actuation}, the time from decision availability to first applied resource change.
In Table~\ref{tab:native-compare}, the \emph{Outcome} column denotes the first
observable control effect after the decision/reaction phase: for HPA, the
replica envelope reached during the run; for VPA, the presence of a generated
recommendation; and for \Solution{}, the frequency of applied replica changes
within the 10-run campaign window.

\begin{table}[t]
  \centering
  \caption{Live CrownLabs comparison across 10 runs (mean \(\pm\) 95\% CI). Time elapsed for Provisioning and Decision/Reaction phases of the auto-scaling control loops and their first observed outcomes. \cz{@PEDRAM: clarify the meaning of the Outcome column and its contents, same as in the Section 4.1.}\pn{Clarified in text right above the table: Outcome means the first observable post-decision control effect (HPA: replica envelope reached; VPA: recommendation produced; AutoSage: applied replica-change frequency across runs). Also reflected in the column header as ``Outcome (first observed effect)''.}}
  \label{tab:native-compare}
  \footnotesize
  \setlength{\tabcolsep}{3pt}
  \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{>{\raggedright\arraybackslash}p{1.2cm}
                  >{\raggedright\arraybackslash}p{1.45cm}
                  >{\raggedright\arraybackslash}p{2.15cm}
                  >{\raggedright\arraybackslash}p{2.55cm}}
    \toprule
    Method & Provisioning & Decision/Reaction & Outcome (first observed effect) \\

    \midrule
    Native HPA & \(0.1849 \pm 0.0138\)\,s & \(26.2407 \pm 6.9430\)\,s to first scale-up & Peak replicas \(6\)--\(8\) observed \\
    Native VPA & \(0.4003 \pm 0.0277\)\,s & \(15.4790 \pm 6.8154\)\,s to first recommendation & Recommendation produced \\
    \Solution{} & N/A & \(126.8182 \pm 31.7235\)\,s to recommendation & Replica change observed in \(3/10\) runs \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[t]
  \centering
  \caption{Service-level and cost-proxy metrics across 10 runs (mean \(\pm\) 95\% CI).}
  \label{tab:service-cost}
  \footnotesize
  \setlength{\tabcolsep}{3pt}
  \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{>{\raggedright\arraybackslash}p{1.65cm}
                  >{\raggedleft\arraybackslash}p{1.15cm}
                  >{\raggedleft\arraybackslash}p{1.15cm}
                  >{\raggedleft\arraybackslash}p{1.55cm}}
    \toprule
    Method & p95 latency (s) & SLA viol. (\%) & Cost proxy (avg req. vCPU) \\
    \midrule
    Native HPA & \(0.0235 \pm 0.0087\) & \(0.0 \pm 0.0\) & \(2.7807 \pm 0.2781\) \\
    Native VPA & \(0.0392 \pm 0.0076\) & \(0.0 \pm 0.0\) & \(1.4140 \pm 0.0000\) \\
    \Solution{} & \(0.0223 \pm 0.0075\) & \(0.0 \pm 0.0\) & \(1.7675 \pm 0.3724\) \\
    \bottomrule
  \end{tabular}
\end{table}
\begin{figure}[t]
  \centering
  \IfFileExists{figures/evaluation/evaluation_decision_latency.pdf}
    {\includegraphics[width=\columnwidth]{figures/evaluation/evaluation_decision_latency.pdf}}
    {\fbox{\parbox{0.95\columnwidth}{\centering Missing file:\\ figures/evaluation/evaluation\_decision\_latency.pdf}}}
  \caption{Decision/reaction latency across native HPA, native VPA, and \Solution{}.}
  \label{fig:evaluation-decision}
\end{figure}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/evaluation/evaluation_service_levels.pdf}
    {\includegraphics[width=\columnwidth]{figures/evaluation/evaluation_service_levels.pdf}}
    {\fbox{\parbox{0.95\columnwidth}{\centering Missing file:\\ figures/evaluation/evaluation\_service\_levels.pdf}}}
  \caption{Service-level metrics from the same 10-run baseline campaign (p95 latency and SLA violation rate).}
  \label{fig:evaluation-service}
\end{figure}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/evaluation/evaluation_cost_proxy.pdf}
    {\includegraphics[width=\columnwidth]{figures/evaluation/evaluation_cost_proxy.pdf}}
    {\fbox{\parbox{0.95\columnwidth}{\centering Missing file:\\ figures/evaluation/evaluation\_cost\_proxy.pdf}}}
  \caption{Cost proxy comparison using average requested vCPU during the evaluation window.}
  \label{fig:evaluation-cost}
\end{figure}

Table~\ref{tab:native-compare} provides a comparison of run times among Native HPA, VPA and AutoSage, for the Provisioning and Decision phases of auto-scaling control loops.
The updated \cz{@PEDRAM: what do you mean by "corrected" here?}\pn{By ``updated'' we mean the final repeated-run baseline after fixing the measurement pipeline and replacing earlier interim/single-run values.} multi-run results indicate a clear trade-off \cz{@PEDRAM: trade-off implies that despite latency being higher, AutoSage performs better than HPA/VPA in some other way. What is this way?}\pn{Here the trade-off is explicit: AutoSage is slower in decision/reaction time, but it shows better service-level/resource-side outcomes than at least one native baseline (lower p95 latency than VPA and lower cost proxy than HPA), while keeping SLA violations at 0\%.}. Native controllers react faster in this short-horizon setup,
while \Solution{} adds decision latency due to LLM inference and MCDA
validation in the control loop. Across the baseline
campaign \cz{@PEDRAM: what do you mean by validated and non-demo? I dont think we need to mention these}\pn{Agreed. We removed ``validated'' and ``non-demo'' from the narrative and keep the wording as ``baseline campaign'' for clarity and brevity.}, \Solution{} produced recommendations in all runs and applied
replica changes in \(3/10\) runs.
\cz{@PEDRAM: clear up the meaning of what you've written in the column "Outcome" in Table 3. "Peak replicas 6–8 observed", "Recommendation produced", "Replica change observed in 3/10 runs". What does each of these mean? What does this information tell us about the performance of AutoSage compared to native scalers? Is AutoSage better than them?}\pn{Meaning of each entry: ``Peak replicas 6--8 observed'' (HPA) means HPA actively scaled and reached 6--8 replicas during the run; ``Recommendation produced'' (VPA) means VPA generated a resource recommendation, but this row does not imply immediate replica count change; ``Replica change observed in 3/10 runs'' (AutoSage) means AutoSage emitted recommendations in all runs, but only 3 runs led to an applied replica-count change within the 60s window. Interpretation: AutoSage is not better than native scalers in control-loop responsiveness/consistency in this short-horizon setup; its strength here is maintaining service-level metrics (0\% SLA violations, competitive p95) rather than faster reaction. Short version: native scalers react faster; AutoSage is more conservative and keeps SLA stable, but does not outperform them on reaction speed in this test.}

We further evaluate AutoSage by assessing service-level quality after each
method run. SLA is measured from request-path probes (20 in-cluster HTTP
requests to the application endpoint per run), and a request is counted as an
SLA violation if either latency exceeds the threshold (\(0.5\)~s) or HTTP
status is \(\ge 400\). Results are shown in Table~\ref{tab:service-cost}.
Service behavior remains stable across all three approaches:
request-level SLA violations remained \(0\%\) for the configured latency threshold (\(0.5\)~s) \cz{@PEDRAM: why 0.5s? justify this choice}\pn{We use 0.5s as an application-level SLO proxy for interactive web response under bursty load: it is strict enough to flag user-visible degradation while remaining realistic for this lightweight HTTP benchmark. It is also applied uniformly across methods to preserve fairness of comparison.}.  
p95 latency stayed in a similar range. This suggests that in the current setup the main differentiator is control-loop policy behavior, rather than request-path latency.
\cz{@PEDRAM: what is the p95 latency here referring to? Which phase of the control loop? How does it differ from the measurements you've made for Table 3?}\pn{The p95 in Table~\ref{tab:service-cost} is request-path latency (tail response time of HTTP probes to the app endpoint), not control-loop timing. Table~\ref{tab:native-compare} instead reports control-loop timing (provisioning and decision/reaction phases). So they measure different layers: user-visible service latency vs autoscaler decision pipeline latency.}

A controlled sensitivity run (\texttt{performance\_first} MCDA profile with
relaxed agreement threshold \(0.05\), same workload and run count \cz{@PEDRAM: describe the test case, what is the agreement threshold (put a reference to the section where you define the meaning of this quantity or briefly recall it here)? Why is it set to 0.05 specifically? What is the purpose of the sensitivity run? Describe and justify these.}\pn{Test case: same 10-run baseline workload, but with MCDA profile switched to \texttt{performance\_first}. Agreement threshold is the maximum score gap under which MCDA accepts the LLM proposal without override; lowering it to 0.05 makes the policy less conservative (fewer overrides). We chose 0.05 as a targeted stress value to test whether reducing MCDA conservativeness improves actuation frequency and end metrics. Purpose: sensitivity analysis to isolate the effect of the MCDA gate on responsiveness and outcome quality.}) increased
the replica-change frequency only marginally (\(4/10\) runs) and did not
improve outcome metrics (\Solution{} p95 latency \(0.0297 \pm 0.0072\)~s vs.
\(0.0223 \pm 0.0075\)~s baseline; cost proxy \(1.9796 \pm 0.4526\) vs.
\(1.7675 \pm 0.3724\) requested vCPU). This indicates that simply relaxing override aggressiveness is insufficient to remove the dominant decision-latency bottleneck.

\subsubsection{Root-Cause Analysis and Improvement Roadmap}
The data supports an important diagnostic conclusion: the primary bottleneck is not SLA-side instability, but control-loop overhead. In both baseline and
sensitivity runs, request-path reliability remained stable (zero SLA violations)
while \Solution{} decision latency stayed substantially above native-controller
reaction times. This behavior is consistent with a pipeline where inference and
validation overhead dominate short-horizon scenarios.

From a scientific perspective, this is still a meaningful result. The study
demonstrates feasibility of an agentic autoscaling pipeline under real cluster
conditions (non-demo metric source, repeated trials, CI reporting), while
identifying where performance headroom remains. The next iteration should
therefore prioritize: \textit{(i)} latency decomposition (LLM inference vs.
MCDA validation vs. Kubernetes reconciliation), \textit{(ii)} asynchronous or
cached decision paths for recurring states, and \textit{(iii)} longer-horizon
periodic-burst workloads where predictive control has a structural advantage
over purely reactive controllers.

\begin{figure}[t]
  \centering
  \IfFileExists{figures/evaluation/evaluation_autosage_decomposition.pdf}
    {\includegraphics[width=\columnwidth]{figures/evaluation/evaluation_autosage_decomposition.pdf}}
    {\fbox{\parbox{0.95\columnwidth}{\centering Missing file:\\ figures/evaluation/evaluation\_autosage\_decomposition.pdf}}}
  \caption{AutoSage decision-latency decomposition (5-run diagnostic campaign; error bars show 95\% CI margin).}
  \label{fig:evaluation-autosage-decomp}
\end{figure}

The decomposition campaign isolates where latency accumulates in \Solution{}.
In the measured run set, metrics collection (\(0.4983 \pm 0.1144\)~s) and LLM
inference (\(1.0008 \pm 0.2178\)~s) are the dominant contributors, while
forecast computation is negligible (\(0.0012 \pm 0.0001\)~s), and actuation
remains comparatively small (\(0.1902 \pm 0.0650\)~s). This supports the
roadmap focus on reducing decision-path overhead (especially inference path and
control-loop orchestration) rather than tuning forecast logic.

Finally, cost is reported here as a \emph{resource proxy}, not direct cloud
billing. We compute it as average requested vCPU over the observed window:
\(\text{avg\_requested\_vcpu}=\text{avg\_replicas}\times
\text{cpu\_request\_per\_pod}/1000\). This captures relative resource footprint
under each method (higher means larger requested CPU footprint), but does not
include provider pricing, discounts, storage, or network charges.

\begin{table}[t]
  \centering
  \caption{CPU forecasting benchmark on simulated application profiles (mean \(\pm\) 95\% CI across seeds/nodes; cell format: MSE / MAPE\%). Lower is better.}
  \label{tab:forecast-autosage-arima-lstm}
  \scriptsize
  \setlength{\tabcolsep}{3pt}
  \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{>{\raggedright\arraybackslash}p{1.4cm}
                  >{\raggedright\arraybackslash}p{2.1cm}
                  >{\raggedright\arraybackslash}p{2.1cm}
                  >{\raggedright\arraybackslash}p{2.1cm}}
    \toprule
    Scenario & ARIMA & \Solution{} predictor & LSTM \\
    \midrule
    Stable API & \(7.69 \pm 5.48\) / \(5.31 \pm 1.27\) & \(9.07 \pm 6.29\) / \(5.69 \pm 1.33\) & \(12.59 \pm 9.17\) / \(7.73 \pm 1.99\) \\
    Bursty Web & \(90.96 \pm 24.23\) / \(18.09 \pm 1.73\) & \(115.24 \pm 32.26\) / \(18.92 \pm 2.66\) & \(145.18 \pm 36.15\) / \(25.93 \pm 2.42\) \\
    Growing Service & \(11.54 \pm 6.28\) / \(5.70 \pm 1.02\) & \(11.92 \pm 7.69\) / \(5.81 \pm 1.30\) & \(23.12 \pm 11.71\) / \(8.70 \pm 1.56\) \\
    \bottomrule
  \end{tabular}
\end{table}

Table~\ref{tab:forecast-autosage-arima-lstm} provides a direct one-step-ahead
forecasting baseline comparison between our current predictor, ARIMA, and an
LSTM network on three simulated application behaviors. ARIMA achieves the
lowest error in all three scenarios, while \Solution{} remains consistently
close to ARIMA and outperforms LSTM on both MSE and MAPE. These scenario-driven
results are more informative than the earlier short live trace and better
reflect predictive behavior under stable, bursty, and drifting workloads.
Based on this benchmark, we adopt ARIMA as the default forecasting backbone in
the current implementation, since it provides the best observed error profile
for this study setting. We emphasize that this choice is empirical (for the
tested data regime and training budget), and future runs with longer traces and
more extensive neural-network tuning may alter the relative ranking.

To evaluate policy quality (separately from forecasting error), we ran a
decision-level ablation with a deterministic oracle and three perturbation
conditions (baseline, noisy metrics, missing-metric injection), comparing
\texttt{llm\_only}, \texttt{mcda\_only}, \texttt{llm\_mcda}, and
\texttt{rule\_only}. The main gain of \texttt{llm\_mcda} is safety: in baseline
condition, bounds-violation rate drops from \(0.674\) (\texttt{llm\_only}) and
\(0.820\) (\texttt{rule\_only}) to \(0.070\), and remains lower under noisy
inputs (\(0.082\) vs \(0.547\) and \(0.663\), respectively). Override analysis
shows that MCDA corrections are usually beneficial (improved-rate range
\(0.73\)–\(0.99\) across scenarios/conditions). At the same time,
\texttt{llm\_mcda} is more conservative (slightly lower macro-F1 than
\texttt{llm\_only}), so the evidence supports a robustness/safety advantage
rather than universal superiority in raw action-matching metrics.

\begin{figure}[t]
  \centering
  \IfFileExists{figures/evaluation/llm_mcda_decision_quality.pdf}
    {\includegraphics[width=\columnwidth]{figures/evaluation/llm_mcda_decision_quality.pdf}}
    {\fbox{\parbox{0.95\columnwidth}{\centering Missing file:\\ figures/evaluation/llm\_mcda\_decision\_quality.pdf}}}
  \caption{Decision-quality comparison under the baseline condition (macro-F1 with 95\% CI).}
  \label{fig:llm-mcda-decision-quality}
\end{figure}

\begin{figure}[t]
  \centering
  \IfFileExists{figures/evaluation/llm_mcda_violation_utility.pdf}
    {\includegraphics[width=\columnwidth]{figures/evaluation/llm_mcda_violation_utility.pdf}}
    {\fbox{\parbox{0.95\columnwidth}{\centering Missing file:\\ figures/evaluation/llm\_mcda\_violation\_utility.pdf}}}
  \caption{Utility and policy-violation trade-off across decision methods (baseline condition).}
  \label{fig:llm-mcda-violation-utility}
\end{figure}

\subsubsection{Evaluation Scope and Limitations}
The presented comparison is now based on repeated runs, but limitations remain:
\textit{(i)} the reported cost is still a resource-based proxy rather than
provider billing data, and \textit{(ii)} the scenario is short-horizon and does
not fully stress periodic/anticipatory benefits of predictive autoscaling.
A more complete study should include longer load profiles with recurring bursts
and explicit decomposition of LLM inference time, MCDA processing time, and
Kubernetes reconciliation time.

\section{Conclusion}
\label{sec:conclusion}

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}

\end{document}
